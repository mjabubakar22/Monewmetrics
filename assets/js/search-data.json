{
  
    
        "post0": {
            "title": "Title",
            "content": "In this notebook, we&#39;ll go through some visualization techniques performed during explanatory data analysis . title :EDA Part 2- toc: true | badges: true | categories:[Statistics &amp; Probability] | image:images/plots.jpg | . While non-graphical analysis provides us with valuable insights, graphical techniques provide a visual representation that enhances our understanding of the data. graphical analysis serves as a vital tool for exploring and understanding data. . Graphical analysis allows us to examine the distribution of data, providing insights into the patterns, shape, and spread of values within a dataset. Histograms, for example, display the frequency or proportion of data points within specific intervals, enabling us to identify peaks, gaps, or skewed distributions. Box plots provide a visual representation of the minimum, maximum, median, and quartiles, helping us understand the central tendencies and variability in the data. By exploring the distribution of data graphically, we can gain a deeper understanding of its characteristics and identify any outliers or anomalies. . Furthermore, Graphical analysis enables us to explore the relationships between different variables in a dataset. Scatter plots, for instance, plot data points as dots on a graph, allowing us to observe the correlation or association between two variables. This helps us identify trends, patterns, or potential dependencies. Additionally, line plots or time series plots provide a visual representation of how variables change over time, highlighting any trends or seasonal patterns. By examining these graphical representations, we can uncover valuable insights into the relationships and dependencies between variables, enabling us to make informed decisions or predictions. . Lastly, Graphical analysis facilitates the comparison of different datasets or categories, allowing us to identify similarities, differences, or trends. Bar charts or column charts, for example, provide a visual representation of categorical data, making it easy to compare the frequency or proportions of different categories. Grouped bar charts or stacked bar charts can be used to compare multiple categories simultaneously. By visually comparing data, we can identify variations, spot outliers, or detect patterns across different groups or time periods. This helps us make data-driven decisions and identify areas of improvement. . By leveraging visual representations, we gain valuable insights into the underlying patterns, trends, and characteristics of the data, empowering us to make informed decisions and draw meaningful conclusions. . We will explore some of the most commonly used visualization tools that I frequently use when performing graphical analysis . import numpy as np import pandas as pd import yfinance as yf from yahoofinancials import YahooFinancials as yfin import plotly.express as px import plotly.graph_objs as go import plotly.figure_factory as ff from IPython.display import HTML from plotly.offline import init_notebook_mode, iplot init_notebook_mode(connected=True) import warnings warnings.simplefilter(&quot;ignore&quot;) . HEATMAP . The correlation heatmap allows for a quick visual assessment of the relationships between variables in a dataset. It helps identify strong positive or negative correlations, patterns, and clusters among variables. This graphical representation provides a more intuitive and comprehensive understanding of the interdependencies among multiple variables compared to a tabular format. . Using the same portfolio of stocks from the previous part, when we examine the return correlation between the stocks, we can quickly observe which stocks move together. As suspected, there is no pair with a negative correlation, considering that all the stocks share common traits. They are all part of the S&amp;P 500, and they all belong to the technology sector, except for Tesla. This explains why Tesla is the only stock that exhibits a less positive correlation with every other stock. However, since Tesla utilizes technology heavily in its electric vehicles, it still considered a tech company by some despite its primary focus on automotive production. Therefore, it exhibits similar characteristics to other tech stocks, which may explain why the correlation is not negative but still not as strong as the correlations among the purely technology-focused stocks. . assets = [&#39;META&#39;,&#39;AMZN&#39;,&#39;NFLX&#39;,&#39;GOOG&#39;,&#39;MSFT&#39;,&#39;NVDA&#39;,&#39;TSLA&#39;] pf_data = pd.DataFrame() for a in assets: pf_data[a] = yf.download(a, start=&quot;2021-10-01&quot;, end=&quot;2021-12-31&quot;)[&#39;Adj Close&#39;] returns = pf_data.pct_change(1).dropna() cov = returns.cov() corr = returns.corr() . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . fig = px.imshow(corr) fig.update_layout(width=1000, height=800) fig.update_layout(template = &quot;plotly_dark&quot;, title = &#39;A heat Map Of Stock Returns In a Portfolio&#39;) fig.show() . SCATTER MATRIX &amp; SCATTER PLOT . Aside from a heatmap we can also use a scatter matrix to check the relationships between the stocks. . A scatter matrix, also known as a scatterplot matrix or pair plot, is a graphical tool used to explore the relationships between multiple variables in a dataset. It consists of a grid of scatterplots, where each scatterplot represents the relationship between two variables. . A scatter plot is a type of graph that displays the relationship between two variables. It consists of a horizontal x-axis and a vertical y-axis, where each data point is represented by a dot or marker on the plot. The position of each dot on the scatter plot corresponds to the values of the two variables being analyzed. . Scatter plots are particularly useful for visualizing the correlation or relationship between two continuous variables. By plotting the data points on the scatter plot, it becomes easier to observe any patterns, trends, or associations between the variables. The general shape or direction of the points on the scatter plot can provide insights into the strength and direction of the relationship. . fig = px.scatter_matrix(returns, title=&#39;A Scatter Matrix Of Stock Returns In A Portfolio&#39;, color_discrete_sequence=[&#39;firebrick&#39;]) fig.update_layout(width=1200, height=800) fig.update_layout(template = &quot;plotly_dark&quot;) fig.show() . When we examine the relationships from both the heatmap and scatter matrix, the pair that exhibits the highest correlation is Microsoft and Google. This can be explained by the fact that these two companies are often mentioned in the same context, as they share similarities in their business models. For instance, Microsoft has Bing while Google dominates the search engine market, both companies have cloud platforms (Azure and Google Cloud Platform), and they offer productivity suites such as Excel and Google Sheets, Word and Google Docs, Gmail and Outlook, among others. To gain a deeper understanding of the relationship between these two stocks, we can visualize their relationship separately and analyze it more closely. . fig = px.scatter(returns, x=&#39;MSFT&#39;, y=&#39;GOOG&#39;, title=&#39;Scatter Plot Of MSFT Return and GOOG Return&#39;,color=&quot;MSFT&quot;)#,trendline=&#39;ols&#39;, trendline_color_override=&#39;firebrick&#39;)#color_discrete_sequence=[&#39;firebrick&#39;]) fig.update_layout(width=1000, height=800) fig.update_layout(template = &quot;plotly_dark&quot;) fig.show() . We can definitely see a linear relationship here, when one does well the other does well and vice versa, this should be taken into consideration whenever one includes both of them in a portfolio . We can also plot a trendline, which is also known as an Ordinary Least Squares (OLS) line. it is often used in scatter plots to depict the overall trend or relationship between two variables. . By using a trendline or OLS line in a scatter plot, we can visually observe the direction and strength of the relationship between the variables. The line is positioned to best represent the general pattern of the data points, whether it be a positive or negative correlation, or even no apparent correlation. . fig = px.scatter(returns, x=&#39;MSFT&#39;, y=&#39;GOOG&#39;, title=&#39;Scatter Plot and Trendline Of MSFT Return and GOOG Return&#39;,color=&quot;MSFT&quot;,trendline=&#39;ols&#39;, trendline_color_override=&#39;firebrick&#39;) fig.update_layout(width=1000, height=800) fig.update_layout(template = &quot;plotly_dark&quot;) fig.show() . In addition to representing the overall trend, a trendline or OLS line in a scatter plot can also be utilized for prediction purposes using linear regression. By fitting a linear regression model to the data, we can establish a mathematical relationship between the variables, enabling us to make predictions or estimates based on this model. . Linear regression allows us to determine the equation of the line that best fits the data points, providing us with a predictive model. This model can then be used to forecast or estimate the value of one variable based on the value of the other variable. By leveraging the linear regression analysis, we can make informed predictions and gain insights into how changes in one variable may impact the other. . But again, caution should be exercised when interpreting the relationship between variables based on a scatter plot and linear regression analysis. Although changes in one variable may be associated with changes in the other variable, it does not necessarily imply causation! . BAR CHART . A bar chart, also known as a bar graph, is a popular visualization tool that presents data using rectangular bars of varying heights. Each bar represents a category or group, and the height of the bar corresponds to the value or frequency of that category. Bar charts are effective in comparing different categories or groups and visually displaying patterns or trends in the data. . For this particular case, I wanted to examine the volume of trades for each stock in the 4th quarter of 2021. As anticipated, Tesla emerged as the leader of the pack, with approximately 5 billion shares traded. Amazon closely followed with 4 billion shares traded. Surprisingly, Netflix has not been receiving the level of trading activity that I would have expected, considering the company&#39;s size and prominence . pf_data2 = pd.DataFrame() for b in assets: pf_data2[b] = yf.download(b, start=&quot;2021-01-01&quot;, end=&quot;2021-12-31&quot;, interval=&#39;3mo&#39;)[&#39;Volume&#39;] . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . fig = px.bar(pf_data2.loc[&#39;2021-10-01&#39;],color_discrete_sequence=[&#39;firebrick&#39;]) fig.update_layout(width=1000, height=800, template = &quot;plotly_dark&quot;) fig.show() . STACKED BAR CHART . A stacked bar chart is a visualization tool that represents different categories or groups of data as stacked bars, where each bar segment corresponds to a subcategory or a portion of the whole . Using the same example, instead of focusing on the volume traded in a single quarter, I now intend to analyze the volume traded in each of the four quarters of 2021. The verdict remains consistent, with Tesla consistently trading the highest volume. Additionally, another noteworthy observation is that the volume for the tech stocks in the portfolio decreased in the 3rd quarter, the volume had been in a downward trend decreasing QoQ since early 2021. . fig = px.bar(pf_data2) fig.update_layout(width=1000, height=800, template = &quot;plotly_dark&quot;) fig.show() . Alternatively, we can plot a side-by-side bar graph for each stock. While a stacked bar chart provides information on the total volume, a side-by-side bar graph presents a clearer depiction of how the volume for each stock has evolved throughout the year. . fig = px.bar(pf_data2,barmode=&#39;group&#39;) fig.update_layout(width=1000, height=800, template = &quot;plotly_dark&quot;) fig.show() . LINE CHART . Line charts, also known as line graphs or time series plots, are effective visual representations for displaying trends and patterns over time. They are particularly useful when analyzing data that is continuous or sequential in nature, such as stock prices, temperature fluctuations, or population growth. . If we plot the stock prices, we can observe the progression of each stock throughout 2021. . fig = px.line(pf_data) fig.update_layout(width=1200, height=800, template = &quot;plotly_dark&quot;) fig.show() . However, I typically examine how each stock has performed relative to one another. To achieve this, I normalize the data using cumulative returns, making it much easier to identify outperforming and underperforming stocks. By doing so, we can see that Nvidia is leading the pack, followed by Tesla, while Netflix and Meta have significantly underperformed in comparison. . cum_returns = (1 + returns).cumprod() - 1 fig = px.line(cum_returns) fig.update_layout(width=1200, height=800, template = &quot;plotly_dark&quot;) fig.show() . PIE CHART . A pie chart is a circular graphical representation that is divided into slices to illustrate the proportional composition of different categories or parts of a whole. Each slice represents a specific category, and its size is determined by the proportion or percentage it contributes to the total. Pie charts are effective in visualizing categorical data and showing the relative sizes or distributions of different categories. They are particularly useful for displaying data that can be grouped into distinct categories and highlighting the relative importance or contribution of each category. By examining a pie chart, we can quickly grasp the overall composition and relative significance of different components within the data. . If we want to determine the sectors that contribute the most or are most represented in the S&amp;P 500, we can use a pie chart for this analysis. From the pie chart, we can observe that there are five dominant sectors, with four of them being cyclical. This observation may explain why the S&amp;P 500 tends to underperform during economic downturns. Additionally, the telecommunication sector has the fewest number of companies in the index, indicating that it is the least represented sector . Data Source: https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information?resource=download . SnP_500 = pd.read_csv(&#39;/home/mj22/data/financials.csv&#39;) fig = px.pie(SnP_500, names=&#39;Sector&#39;) fig.update_layout(width=1200, height=800, template = &quot;plotly_dark&quot;) fig.show() . BOX PLOT . A box plot is a graphical representation that provides a visual summary of the distribution of a dataset. It is particularly useful for comparing the distribution of multiple variables or groups. The plot consists of a box that represents the interquartile range (IQR), which contains the middle 50% of the data, with a line inside the box indicating the median. The whiskers extend from the box to the minimum and maximum values, excluding any outliers, which are represented as individual points. By examining the box plot, we can identify the central tendency, spread, skewness, and potential outliers in the data, making it a powerful tool for exploratory data analysis. . When we examine the box plot for Price/Sales for all the stocks in the index, we quickly notice the presence of outliers. The maximum value is 20, the minimum value is 0.15, and the median is 2.89. The first quartile is 1.62, and the third quartile is 4.71. . fig = px.box(SnP_500, y=&#39;Price/Sales&#39;, color_discrete_sequence=[&#39;firebrick&#39;]) fig.update_layout(width=1000, height=800, template = &quot;plotly_dark&quot;) fig.show() . We can also analyze the Price/Sales ratio for each sector in the index, providing us with a more insightful observation. From this analysis, we can observe that the real estate sector has a significantly higher P/S ratio compared to other sectors, while the telecommunications sector appears to have the lowest. However, it&#39;s important to note that this is an inductive observation, as the index only includes the 500 largest companies, which may not present the complete picture. . fig = px.box(SnP_500, y=&#39;Price/Sales&#39;, x=&#39;Sector&#39;, color_discrete_sequence=[&#39;firebrick&#39;]) fig.update_layout(width=1200, height=800, template = &quot;plotly_dark&quot;) fig.show() . VIOLIN PLOT . Alternatively we can use a violin plot, a violin plot combines a box plot with a kernel density plot. It displays the same summary statistics as a box plot, but also provides a more detailed view of the distribution. The plot is symmetrical and resembles a violin or a mirrored density plot. The width of the violin at each point represents the density of data points, with wider areas indicating higher density. . fig = px.violin(SnP_500, y=&#39;Earnings/Share&#39;, color_discrete_sequence=[&#39;firebrick&#39;]) fig.update_layout(width=1000, height=800, template = &quot;plotly_dark&quot;) fig.show() . Compared to box plots, violin plots offer additional insights into the shape and multimodality of the distribution. They provide a more comprehensive visualization of the data, allowing for a better understanding of its characteristics. However, box plots are more compact and straightforward, making them useful for quick comparisons between multiple groups or variables. . Additionally, with a violin plot, we can plot scatter dots alongside to better visualize the concentration of data. . We can see that most of the companies in the index have earnings per share between 12 and -3, but the data is more concentrated around the range of 3 to 1. Additionally, the majority of companies have a positive earnings per share. . fig = px.violin(SnP_500, y=&#39;Earnings/Share&#39;, color_discrete_sequence=[&#39;firebrick&#39;], points=&quot;all&quot;) fig.update_layout(width=1000, height=800, template = &quot;plotly_dark&quot;) fig.show() . Again, we could also observe the earnings per share (EPS) for all the sectors. It appears that almost all of the sectors have a higher number of companies with positive earnings, except for the energy sector. This discrepancy raises further questions and warrants investigation to understand why the energy sector has more companies with negative earnings compared to the rest. . fig = px.violin(SnP_500, y=&#39;Earnings/Share&#39;, x=&#39;Sector&#39;, color_discrete_sequence=[&#39;firebrick&#39;],points=&quot;all&quot;) fig.update_layout(width=1200, height=800, template = &quot;plotly_dark&quot;) fig.show() . Both violin plots and box plots serve as valuable tools in exploratory data analysis, helping to identify central tendencies, dispersion, skewness, and potential outliers in a dataset. The choice between the two depends on the specific requirements and the level of detail desired in visualizing the data distribution. . Histogram . A histogram is a graphical representation that displays the distribution of a dataset. It consists of a series of bars, where each bar represents a range of values and the height of the bar corresponds to the frequency or count of observations falling within that range. Histograms provide a visual depiction of the data&#39;s frequency distribution, allowing us to identify patterns, skewness, and central tendencies. They are particularly useful for understanding the shape of the data and detecting any outliers or unusual patterns. By examining the histogram, we can gain insights into the underlying characteristics and distribution of the variable being analyzed . When we plot the weekly distribution of returns for the MSCI World Index, we can observe that most of the data points are situated around -1.5% to 2.5%. Additionally, we can identify outliers scattered along the distribution, indicating extreme or unusual returns compared to the majority of data points. . world = yf.download(&#39;IXUS&#39;, &quot;2013-01-01&quot;, &quot;2022-06-01&quot;,interval=&quot;1wk&quot;)[&#39;Adj Close&#39;] world_returns = world.pct_change(1).dropna() . [*********************100%***********************] 1 of 1 completed . fig = px.histogram(world_returns, x=world_returns, nbins=80, color_discrete_sequence=[&#39;firebrick&#39;]) # Customize the layout if needed fig.update_layout(title=&quot;Distribution of Returns&quot;, xaxis_title=&quot;Returns&quot;, yaxis_title=&quot;Count&quot;,width=1500, height=900,template = &quot;plotly_dark&quot;) fig.show() . But the way I always use the distribution of returns is by analyzing the percentage of occurrence for each bin, which provides insights into the potential expectations for the asset going forward. Interpreting the data, we can derive the following conclusions: . In approximately 66% of the time, the MSCI returns anywhere from -1.5% to +2.5% in a given week. | If you were looking to go long on the index with a target return of 3%, the probability of achieving that would be 5.4%. | Conversely, if you were aiming to go short and expecting at least a 2% return, the probability of achieving that return is approximately 9%. However, both probabilities are statistically insignificant. | . From this analysis, we can conclude that expecting a 3% return or a 2% return is unrealistic, and it may be necessary to adjust your target returns accordingly. Let&#39;s consider targeting a 1% return instead: . If you were to go long with a minimum target of a 1% return, the probability of that happening would be 33%. | On the other hand, if you were to go short with a minimum target of a 1% return, the probability of achieving that desired return would be 23%. | . Although the odds appear to be better when adjusting our expectations, it is still statistically insignificant. Comparing it to a skydiving scenario where the parachute only works 33% of the time, taking such a risk would not be advisable. . fig = px.histogram(world_returns, nbins=80, color_discrete_sequence=[&#39;firebrick&#39;], text_auto=True, histnorm=&#39;percent&#39;) # Customize the layout if needed fig.update_layout(title=&quot;Distribution of Returns&quot;, xaxis_title=&quot;Returns&quot;, yaxis_title=&quot;Count&quot;,width=1500, height=900,template = &quot;plotly_dark&quot;) fig.show() . Histograms are a great tool for exploring data. However, there is still more to uncover. As you may have noticed, we have begun delving into the realm of probability. In our next blog, we will continue this exploration so that we can gain a deeper understanding of distributions and appreciate concepts such as skewness, fat tails, probability of events, and more. By incorporating these theories, we will further enhance our data analysis capabilities. . References . &quot;Quantitative Investment Analysis&quot;, by DeFusco, McLeavey, Pinto, and Runkle | &quot;Introduction to Modern Statistics&quot;, by Mine Çetinkaya-Rundel and Johanna Hardin | .",
            "url": "https://mjabubakar22.github.io/Monewmetrics/2022/10/07/Stats2.html",
            "relUrl": "/2022/10/07/Stats2.html",
            "date": " • Oct 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "In this notebook, we&#39;ll go through some basic concepts that most if not all statisticians and data analysts look at when they first encouter a data set . title :EDA Part 1- toc: true | badges: true | categories:[Statistics &amp; Probability] | image:images/math.jpg | . Exploratory Data Analysis (EDA) is a systematic and unbiased approach to analyze and understand a dataset by employing statistical techniques and visualizations. It involves thoroughly examining the dataset from various perspectives, summarizing its characteristics, and identifying key patterns, trends, or anomalies without making any prior assumptions about the data&#39;s nature or underlying relationships. EDA aims to provide meaningful insights and uncover relevant features of the data that can guide further analysis and decision-making processes. . There are two types of data, categorical and numerical data. . Categorical data, also known as qualitative data, is a type of data that represents characteristics or attributes that belong to a specific category or group . While, Numerical data, also known as quantitative data, is a type of data that consists of numeric values representing measurable quantities or variables. . Univariate Analysis: Univariate analysis is a type of exploratory data analysis that focuses on analyzing or dealing with only one variable at a time. It involves examining and describing the data of a single variable, aiming to identify patterns and characteristics within that variable. Univariate analysis does not consider causes or relationships and primarily serves the purpose of providing insights and summarizing data. . Bivariate Analysis: Bivariate analysis is a type of exploratory data analysis that involves the analysis of two different variables simultaneously. It explores the relationship between these two variables, aiming to understand how changes in one variable may impact the other. Bivariate analysis delves into causality and relationships, seeking to identify associations and dependencies between the two variables under investigation. . Multivariate Analysis: Multivariate analysis is a type of exploratory data analysis that deals with datasets containing three or more variables. It examines the relationships and patterns between multiple variables, allowing for a more comprehensive analysis of the data. Multivariate analysis employs various statistical techniques and graphical representations to uncover complex relationships and interactions among the variables, facilitating a deeper understanding of the dataset as a whole. . EDA consists of two parts, . Non-graphical Analysis and Graphical Analysis Non-graphical Analysis: Non-graphical analysis in exploratory data analysis involves examining and analyzing data using statistical tools and measures such summary statistic that quantitatively describes or summarizes features of a dataset. It focuses on understanding the characteristics and patterns for mostly one variable but can also be used for two or more variables. . Graphical Analysis: Graphical analysis is the most common part of exploratory data analysis that utilizes visualizations and charts to analyze and interpret data. It involves representing data in graphical forms to visually identify trends, patterns, distributions, relationships between variables or even compare different variables. Graphical analysis provides a comprehensive view of the data, allowing for a better understanding of the underlying structure and facilitating the exploration of multivariate relationships. . We&#39;ll start off by first performing the non graphical part then we will finish with graphical analysis in the second part . Measures of central tendency: . A measure of central tendency (also referred to as measures of center or central location) is a summary measure that attempts to describe a whole set of data with a single value that represents the middle or center of its distribution. . Mean: . The arithmetic mean is the sum of the observations divided by the number of observations. The arithmetic mean is by far the most frequently used measure of the middle or center of data. The mean is also referred to as the average The population mean, μ, is the arithmetic mean value of a population. For a finite population, the population mean is: . $$ mu = dfrac{ sum_{i=1}^N X_i}{N} $$ . where $N$ is the number of observations in the entire population and $X_i$ is the $i$th observation. . The sample mean is the arithmetic mean computed for a sample. A sample is a percentage of the total population in statistics. You can use the data from a sample to make inferences about a population as a whole. The concept of the mean can be applied to the observations in a sample with a slight change in notation. . $$ bar{x} = dfrac{ sum_{i=1}^n X_i}{n} $$ . where $n$ is the number of observations in the sample. . import pandas as pd import numpy as np import yfinance as yf import scipy.stats as stats import statistics . msft_daily = yf.download(&#39;MSFT&#39;, &#39;2016-01-01&#39;, &#39;2021-12-31&#39;, interval= &#39;1d&#39;)[&#39;Adj Close&#39;] . msft_returns = msft_daily.pct_change(1).dropna() average_return = str(round((np.mean(msft_returns) * 100),2)) + &#39;%&#39; print(f&#39;The average daily return for Microsoft stock is: {average_return}&#39;) . The average daily return for Microsoft stock is: 0.14% . Weighted mean: . The ordinary arithmetic mean is where all sample observations are equally weighted by the factor 1/n (each of the data points contributes equally to the final average). . But with the weighted mean, some data points contribute more than others based on their weighting, the higher the weighting, the more it influences the final average. The weighted mean is also referred to as the weighted average . $$ bar{X}_w = { sum_{i=1}^n w_i X_i} $$ . where the sum of the weights equals 1; that is,$ sum_{i} w_i = 1$ . aapl = yf.download(&#39;AAPL&#39;, &#39;2021-10-01&#39;, &#39;2021-12-31&#39;, interval= &#39;1d&#39;)[&#39;Adj Close&#39;] nvdia = yf.download(&#39;NVDA&#39;, &#39;2021-10-01&#39;, &#39;2021-12-31&#39;, interval= &#39;1d&#39;)[&#39;Adj Close&#39;] msft = yf.download(&#39;MSFT&#39;, &#39;2021-10-01&#39;, &#39;2021-12-31&#39;, interval= &#39;1d&#39;)[&#39;Adj Close&#39;] . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . msft_ret = (msft[-1] - msft[0])/msft[0] aapl_ret = (aapl[-1] - aapl[0])/aapl[0] nvda_ret = (nvdia[-1] - nvdia[0])/aapl[0] #portfolio return if 50% of capital was deplyoed in microsoft,30% in apple and 20% in nvidia Wavg = (msft_ret * .50 + aapl_ret + .30 + nvda_ret * .20)/3 avg = (msft_ret + aapl_ret + nvda_ret)/3 weighted = str(round(Wavg,2)) + &#39;%&#39; arith = str(round(avg,2)) + &#39;%&#39; print(f&quot;The Weighted mean return of the portfolio assuming a 50/30/20 split is: {weighted}&quot;) print(f&quot;The Arithmetic mean return of the portfolio assuming no split is:&quot;, arith) . The Weighted mean return of the portfolio assuming a 50/30/20 split is: 0.25% The Arithmetic mean return of the portfolio assuming no split is: 0.35% . The weighted mean is also very useful when calculating a theoretically expected outcome where each outcome has a different probability of occurring (more on this in probability concepts) . Harmonic mean: . The harmonic mean is a type of numerical average. It is calculated by dividing the number of observations by the reciprocal of each number in the series. Thus, the harmonic mean is the reciprocal of the arithmetic mean of the reciprocals. . $$ bar{X}_h = dfrac{n}{ sum_{i=1}^n dfrac1X_i} $$ . Geometric mean: . The geometric mean is most frequently used to average rates of change over time or to compute the growth rate of a variable. . For volatile numbers like stock returns, the geometric average provides a far more accurate measurement of the true return by taking into account year-over-year compounding that smooths the average. . $$ G = sqrt[n]{X_1,X_2,X_3....X_n} $$ . Data Source: https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information?resource=download . SnP_500 = pd.read_csv(&#39;/home/mj22/data/financials.csv&#39;) SnP_500 . Symbol Name Sector Price Price/Earnings Dividend Yield Earnings/Share 52 Week Low 52 Week High Market Cap EBITDA Price/Sales Price/Book SEC Filings . 0 MMM | 3M Company | Industrials | 222.89 | 24.31 | 2.332862 | 7.92 | 259.77 | 175.490 | 1.387211e+11 | 9.048000e+09 | 4.390271 | 11.34 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 1 AOS | A.O. Smith Corp | Industrials | 60.24 | 27.76 | 1.147959 | 1.70 | 68.39 | 48.925 | 1.078342e+10 | 6.010000e+08 | 3.575483 | 6.35 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 2 ABT | Abbott Laboratories | Health Care | 56.27 | 22.51 | 1.908982 | 0.26 | 64.60 | 42.280 | 1.021210e+11 | 5.744000e+09 | 3.740480 | 3.19 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 3 ABBV | AbbVie Inc. | Health Care | 108.48 | 19.41 | 2.499560 | 3.29 | 125.86 | 60.050 | 1.813863e+11 | 1.031000e+10 | 6.291571 | 26.14 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 4 ACN | Accenture plc | Information Technology | 150.51 | 25.47 | 1.714470 | 5.44 | 162.60 | 114.820 | 9.876586e+10 | 5.643228e+09 | 2.604117 | 10.62 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 500 XYL | Xylem Inc. | Industrials | 70.24 | 30.94 | 1.170079 | 1.83 | 76.81 | 46.860 | 1.291502e+10 | 7.220000e+08 | 2.726209 | 5.31 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 501 YUM | Yum! Brands Inc | Consumer Discretionary | 76.30 | 27.25 | 1.797080 | 4.07 | 86.93 | 62.850 | 2.700330e+10 | 2.289000e+09 | 6.313636 | 212.08 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 502 ZBH | Zimmer Biomet Holdings | Health Care | 115.53 | 14.32 | 0.794834 | 9.01 | 133.49 | 108.170 | 2.445470e+10 | 2.007400e+09 | 3.164895 | 2.39 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 503 ZION | Zions Bancorp | Financials | 50.71 | 17.73 | 1.480933 | 2.60 | 55.61 | 38.430 | 1.067068e+10 | 0.000000e+00 | 3.794579 | 1.42 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 504 ZTS | Zoetis | Health Care | 71.51 | 32.80 | 0.682372 | 1.65 | 80.13 | 52.000 | 3.599111e+10 | 1.734000e+09 | 9.280896 | 18.09 | http://www.sec.gov/cgi-bin/browse-edgar?action... | . 505 rows × 14 columns . print(&quot;The Harmonic mean of Price to Sales Ratio for companies in the S&amp;P 500 is:&quot;, round(stats.hmean(SnP_500[&#39;Price/Sales&#39;]),2)) print(&quot;The Geometric mean of Price to Sales Ratio for companies in the S&amp;P 500 is:&quot;, round(stats.gmean(SnP_500[&#39;Price/Sales&#39;]),2)) print(&quot;The Arithmetic mean of Price to Sales Ratio for companies in the S&amp;P 500 is:&quot;, round(np.mean(SnP_500[&#39;Price/Sales&#39;]),2)) . The Harmonic mean of Price to Sales Ratio for companies in the S&amp;P 500 is: 1.95 The Geometric mean of Price to Sales Ratio for companies in the S&amp;P 500 is: 2.83 The Arithmetic mean of Price to Sales Ratio for companies in the S&amp;P 500 is: 3.94 . “A mathematical fact concerning the harmonic, geometric, and arithmetic means is that unless all the observations in a data set have the same value, the harmonic mean is less than the geometric mean, which in turn is less than the arithmetic mean” – Quantitative Investment Analysis, by DeFusco, McLeavey, Pinto, and Runkle . Trimmed mean: . A trimmed mean is a method of averaging that removes a small designated percentage of the largest and smallest values before calculating the mean. After removing the specified outlier observations, the trimmed mean is found using a standard arithmetic averaging formula. The use of a trimmed mean helps eliminate the influence of outliers or data points on the tails that may unfairly affect the traditional or arithmetic mean. . To trim the mean by a total of 40%, we remove the lowest 20% and the highest 20% of values, eliminating the scores of 8 and 9 . data = [8, 2, 3, 4, 9] mean = np.mean(data) trimmed_data = [2, 3, 4] trimmed_mean = np.mean(trimmed_data) print(f&quot;Hence, a mean trimmed at 40% would equal {trimmed_mean} versus {mean}&quot;) . Hence, a mean trimmed at 40% would equal 3.0 versus 5.2 . Median: . The median is the middle number in a sorted, ascending, or descending list of numbers and can be more descriptive of that data set than the average. It is the point above and below which half (50%) of the observed data falls, and so represents the midpoint of the data. . print(f&#39;The Median Price to Sales Ratio for Companies in the S&amp;P 500 is:&#39;, round(np.median(SnP_500[&#39;Price/Sales&#39;]),2)) . The Median Price to Sales Ratio for Companies in the S&amp;P 500 is: 2.9 . Mode: . The mode is the value that appears most frequently in a data set A distribution can have more than one mode or even no mode. When a distribution has one most frequently occurring value, the distribution is said to be unimodal. If a distribution has two most frequently occurring values, then it has two modes and we say it is bimodal. If the distribution has three most frequently occurring values, then it is trimodal. When all the values in a data set are different, the distribution has no mode because no value occurs more frequently than any other value. . mode = round(statistics.mode(SnP_500[&#39;Price/Sales&#39;]),2) print(f&#39;The Mode of the Price to Sales ratio for Companies in the S&amp;P 500 is {mode}, indicating that it is the most commonly occurring value among the dataset&#39;) . The Mode of the Price to Sales ratio for Companies in the S&amp;P 500 is 4.39, indicating that it is the most commonly occurring value among the dataset . Measures of dispersion . Location is just one dimension in describing data. A second dimension, variability, also referred to as dispersion, measures whether the data values are tightly clustered or spread out. . Range: . Range, the range of a set of data is the difference between the largest and smallest values, the result of subtracting the sample maximum and minimum. It is expressed in the same units as the data. . maX = np.max(dividends) miN = np.min(dividends) Range= np.ptp(dividends) print(f&#39;The maximum dividend per share paid by Microsoft is {maX}$&#39;) print(f&#39;The minimum dividend per share paid by Microsoft is {miN}$&#39;) print(f&#39;Hence, the range is {Range}$&#39;) . The maximum dividend per share paid by Microsoft is 3.08$ The minimum dividend per share paid by Microsoft is 0.08$ Hence, the range is 3.0$ . Variance sample and population: . The variance and standard deviation are the two most widely used measures of dispersion Variance is defined as the average of the squared deviations around the mean. Population variance is a measure of the spread of population data. Hence, population variance can be defined as the average of the distances from each data point in a particular population to the mean squared, and it indicates how data points are spread out in the population. we can compute the population variance.Denoted by the symbol σ2 . Population formula: . $$ sigma^2 = dfrac{ sum_{i=1}^N(x_i - mu)^2}N $$ . While sample formula is: . $$s^2 = dfrac{ sum_{i=1}^n(x_i - bar x)^2}{n-1} $$ . Standard deviation sample and population: . Standard Deviation (SD) is the positive square root of the variance. It is represented by the Greek letter ‘σ’ and is used to measure the amount of variation or dispersion of a set of data values relative to its mean (average), thus interpret the reliability of the data. If it is smaller then the data points lie close to the mean value, thus showing reliability. But if it is larger then data points spread far from the mean. . Population formula: . $$ sigma^2 = sqrt{ dfrac{ sum_{i=1}^N(x_i - mu)^2}N} $$ . While sample formula is: . $$s^2 = sqrt{ dfrac{ sum_{i=1}^n(x_i - bar x)^2}{n-1}} $$ . Hence, Variance Measures Dispersion within the Data Set while the standard deviation measures spread around the mean! . print(&quot;The variance of Microsoft&#39;s daily stock returns:&quot;, round(np.var(msft_returns),5)) print(&quot;The standard deviation of Microsoft&#39;s daily stock returns:&quot;, round(np.std(msft_returns),5)) . The variance of Microsoft&#39;s daily stock returns: 0.00028 The standard deviation of Microsoft&#39;s daily stock returns: 0.01684 . Relationship between variables . Relationship between variables means, In a dataset, the values of one variable correspond to the values of another variable. . By conducting a non-graphical analysis of the relationship between variables, you can quantitatively assess their associations, dependencies, and impacts, providing valuable insights for further analysis and decision-making . Moreover, non-graphical analysis of the relationship between variables involves examining the numerical values in a matrix to understand the connections between variables. A covariance matrix and a correlation matrix are square matrices that display the pairwise relationships between different variables in a dataset. They provide valuable insights into the strength and direction of the relationships between variables . Covariance . Covariance provides insight into how two variables are related to one another. More precisely, covariance refers to the measure of how two random variables in a data set will change together. A positive covariance means that the two variables at hand are positively related, and they move in the same direction. A negative covariance means that the variables are inversely related, or that they move in opposite directions. Both variance and covariance measure how data points are distributed around a calculated mean. However, variance measures the spread of data along a single axis, while covariance examines the directional relationship between two variables. . Population formula: . $$ sigma_{xy} = dfrac{ sum_{i=1}^N(x_i - mu_x)*(y_i - mu_y)}N $$ . While sample formula is: . $$s_{xy} = dfrac{ sum_{i=1}^n(x_i - bar x)*(y_i - bar y)}{n-1} $$ . assets = [&#39;META&#39;,&#39;AMZN&#39;,&#39;NFLX&#39;,&#39;GOOG&#39;,&#39;MSFT&#39;,&#39;NVDA&#39;,&#39;TSLA&#39;] pf_data = pd.DataFrame() for a in assets: pf_data[a] = yf.download(a, start=&quot;2021-10-01&quot;, end=&quot;2021-12-31&quot;, index_col = &#39;Date&#39;, parse_dates=True)[&#39;Adj Close&#39;] returns = pf_data.pct_change(1).dropna() cov = returns.cov() corr = returns.corr() . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . cov . META AMZN NFLX GOOG MSFT NVDA TSLA . META 0.000404 | 0.000137 | 0.000143 | 0.000135 | 0.000134 | 0.000252 | 0.000189 | . AMZN 0.000137 | 0.000253 | 0.000075 | 0.000142 | 0.000111 | 0.000362 | 0.000064 | . NFLX 0.000143 | 0.000075 | 0.000319 | 0.000098 | 0.000115 | 0.000196 | 0.000151 | . GOOG 0.000135 | 0.000142 | 0.000098 | 0.000221 | 0.000177 | 0.000284 | 0.000074 | . MSFT 0.000134 | 0.000111 | 0.000115 | 0.000177 | 0.000207 | 0.000287 | 0.000182 | . NVDA 0.000252 | 0.000362 | 0.000196 | 0.000284 | 0.000287 | 0.001286 | 0.000490 | . TSLA 0.000189 | 0.000064 | 0.000151 | 0.000074 | 0.000182 | 0.000490 | 0.001463 | . Correlation coefficient . Correlation shows the strength of a relationship between two variables and is expressed numerically by the correlation coefficient. While covariance measures the direction of a relationship between two variables, correlation measures the strength of that relationship. There are many different measures of correlation but the most common one, and the one I use is the Pearson Coefficient of Correlation. . Output values of the Pearson Correlation Coefficient range between values of +1 and -1, or 100% and -100%, where +1 represents perfect positive correlation and -1 perfect negative correlation. A measure of 0 would suggest the two variables are perfectly uncorrelated, and there is no linear relationship between them. However, that doesn’t necessarily mean the variables are independent – as they might have a relationship that is not linear. Scatterplot charts are a good way of visualizing various values for correlation . Population formula: . $$p = dfrac{ sigma_{xy}}{ sigma_y sigma_x} $$ . While sample formula is: . $$r = dfrac{s_{xy}}{s_ys_x} $$ . corr . META AMZN NFLX GOOG MSFT NVDA TSLA . META 1.000000 | 0.426954 | 0.399251 | 0.451141 | 0.462463 | 0.349047 | 0.245190 | . AMZN 0.426954 | 1.000000 | 0.263913 | 0.599965 | 0.486596 | 0.635210 | 0.105951 | . NFLX 0.399251 | 0.263913 | 1.000000 | 0.369411 | 0.449536 | 0.306624 | 0.221220 | . GOOG 0.451141 | 0.599965 | 0.369411 | 1.000000 | 0.829273 | 0.532455 | 0.130574 | . MSFT 0.462463 | 0.486596 | 0.449536 | 0.829273 | 1.000000 | 0.556526 | 0.331061 | . NVDA 0.349047 | 0.635210 | 0.306624 | 0.532455 | 0.556526 | 1.000000 | 0.357039 | . TSLA 0.245190 | 0.105951 | 0.221220 | 0.130574 | 0.331061 | 0.357039 | 1.000000 | . One of the most common pitfalls of correlation analysis is that correlation is not causation! . Just because two variables have shown a historic correlation doesn’t mean that one of the variables causes the other to move. The causation of the two variables moving with a positive or negative correlation could be a third completely unconsidered variable OR a combination of many factors. In theory, we want to try and understand the causes for relationships between variables so we can have a more accurate idea about when those relationships might change and if they will. The reality is that this is very hard to achieve and so practically speaking correlation analysis is often used to summarise relationships and use them as forward-looking predicator under the caveat that we understand it is likely that there are many factors at play that are responsible for the causation of the relationship. . References . &quot;Quantitative Investment Analysis&quot;, by DeFusco, McLeavey, Pinto, and Runkle | &quot;Practical Statistics for Data Scientists&quot;, by Andrew Bruce, Peter C. Bruce, and Peter Gedeck | .",
            "url": "https://mjabubakar22.github.io/Monewmetrics/2022/08/21/Stats1.html",
            "relUrl": "/2022/08/21/Stats1.html",
            "date": " • Aug 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Stock Price Forecast Using ARIMA Model",
            "content": "DISCLAIMER! . Before proceeding, please make sure that you note the following important information: . NOT FINANCIAL ADVICE! . My content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice, None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument to make any investment or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur . Intro on time series: . A time series is a sequence of data that attaches a time period to each value, the value can be anything measurable that depends on time in some way like rainfall measurements, heart rate measurements, annual retail sales, monthly subscription, trading volumes, security prices, exchange rates and so on. . All we need for a time series is a starting point and an ending point(a time period), all time periods must be equal and clearly defined which would result in a constant frequency. The frequency is how often the values of the data set are recorded which could be hourly, daily, monthly, quarterly, etc. There are also no limitations regarding the total time span of our time series. . Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics like observing patterns in the data while Time series forecasting is the use of a model to predict future values based on previously observed patterns. . Data can be univariate or multivariate, univariate time series is when we are forecasting the value of a single variable based on patterns observed in its own past history. For example, predicting the closing stock price of apple tomorrow using its own closing price in the past 7 trading days. While multivariate time series is when we are forecasting the value of a single variable considering patterns in parallel time series. For example, predicting the closing stock price of apple tomorrow using the closing price of Microsoft and Nvidia in the past 7 trading days. . import numpy as np import pandas as pd import yfinance as yf import statsmodels.api as sm from statsmodels.tsa.stattools import adfuller from statsmodels.tsa.arima.model import ARIMA from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf from pmdarima.arima.utils import ndiffs import datetime from pandas_datareader import data as wb import matplotlib.pyplot as plt import warnings warnings.simplefilter(&quot;ignore&quot;) %matplotlib inline . Objective: . The American economy contracted an annualized 1.6% in Q1 2022. It was the first contraction since the pandemic-induced recession in 2020 as record trade deficits, supply constraints, worker shortages, and high inflation weigh. . Year to date, the S&amp;P500 is down approximately 20% this officially puts it in a bear market territory. As fear of recession weighs in, historically the consumer staple sector usually fare well during these downturns as the idea is that companies in these sectors produce goods &amp; services that consumers will buy regardless of the economic conditions. . Hence, the objective of today’s blog is to forecast the stock price of The Procter &amp; Gamble Company (Ticker PG) using the Auto Regressive Integrated Moving Average (ARIMA) model. . The reasons why I picked PG is because they have a diverse pool of products mainly 5 segments, Beauty, Grooming, Health Care, Fabric &amp;Home Care, and Baby, Feminine &amp; Family Care, they also operate in a lot of regions with consumers from Latin America, Europe, the Asia Pacific, Greater China, India, the Middle East, and Africa. . NOTE: depending on the level of risk one is willing to take with his/her portfolio, a safer bet would be to trade the overall sector using an ETF like the Consumer Staples Select Sector SPDR Fund(Ticker XLP) or Vanguard Consumer Staples ETF(Ticker VDC) in order to get exposure of the sector in one&#39;s portfolio. . What is an ARIMA model: . ARIMA stands for Autoregressive Integrated Moving Average. ARIMA is also known as the Box-Jenkins approach. it is a combination of an AR model, MA model, and differencing (Integration). . The components of an ARIMA model: . In an ARIMA(p,d,q) model, p indicates the number or order of AR terms, d indicates the number or order of differences, and q indicates the number or order of MA terms. The p, d, and q parameters are integers equal to or greater than 0 . AR: . A correlation of a variable with itself at different time periods in the past is known as “Autocorrelation”. AR model uses past values to make future predictions. It is indicated by the “p” value in the ARIMA model. The lag “p” term signifies how many prior time periods that each observation is highly correlated to, Increasing “p” (longer lag) would increase the dependency on previous values further. . if $Y$ is a time series variable and AR(2) then the equation would look like this: . $Y_{t} = c + varphi_{1}Y_{t&#8722;1} + varphi_{2}Y_{t&#8722;2} + &#1013;_{t}$ . Where: . $c$ is a constant | $Y_{t−1}$ is the values of $Y$ during the previous period | $ varphi$ is the magnitude of the autocorrelation | $ϵ_{t}$ is the residuals for the current period (the difference between our prediction for period $_{t}$ and the correct value) | . MA: . MA model uses past errors to make a prediction. The “q” term is the number of lagged values of the error term it . if $Y$ is a time series variable and MA(2) then the equation would look like this: . $Y_{t} = c + &#952;_{1}&#1013;_{t&#8722;1} + &#952;_{2}&#1013;_{t&#8722;2} + &#1013;_{t}$ . Where: . $c$ is a constant | $θ$ is the value of the autocorrelation of the error | $ϵ_{t}$ is the residuals for the current period | $ϵ_{t−1}$ is the residuals for the past period | . I: . The I stands for Integrated and is used to difference the time series data to remove the trend and convert a non-stationary time series to a stationary one. This is indicated by the “d” value in the ARIMA model. Hence we are combining AR and MA techniques into a single integrated model in order to achieve weak stationarity . if $Y$ is a time series variable, then first order differencing equation would look like: . $Y_{t}^&#8242; = Y_{t} &#8722; Y_{t&#8722;1}$ . Where: . $Y_{t}^′$ is the difference between adjacent observation | . ARIMA equation: . Thus if $Y$ is a time series variable and ARIMA(2,1,2) then the equation would be: . $Y_{t}^&#8242; = c + varphi_{1}Y_{t&#8722;1}^&#8242; + varphi_{2}Y_{t&#8722;2}^&#8242; + &#952;_{1}&#1013;_{t&#8722;1} + &#952;_{2}&#1013;_{t&#8722;2} + &#1013;_{t}$ . An ARIMA model with 0 degrees of integration is simply an ARMA model, and so any ARIMA (p, 0, q) model is equivalent to an ARMA (p,q). likewise, any ARIMA(p, 0, 0) is equivalent to an AR(p) model and any ARIMA(0, 0, q) is equivalent to an MA(q) model . EDA: . The objective now is to perform an Exploratory Data Analysis(EDA) on the dataset (Closing prices of PG) to find the values of p, q, and d . Before performing EDA, I&#39;m going to split the data first and work only with the train data set which I will use to perform the EDA on, while the test data will be used to evaluate the prediction . df = yf.download(&#39;PG&#39;, &#39;2018-06-01&#39;, &#39;2022-06-30&#39;, interval= &#39;1d&#39;, auto_adjust=True) # Only keep close column df = df[[&#39;Close&#39;]] # Drop rows with missing values df = df.dropna() #make a copy of my data just incase i need the original data df_copy = df.copy() #Setting frequency to days df.index = pd.DatetimeIndex(df.index).to_period(&#39;D&#39;) . [*********************100%***********************] 1 of 1 completed . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; PeriodIndex: 1028 entries, 2018-05-31 to 2022-06-29 Freq: D Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 Close 1028 non-null float64 dtypes: float64(1) memory usage: 16.1 KB . 90/10 split, 90% of data will be used to train and 10% to test . t = .9 t = int(t*len(df)) # Train dataset train = df[:t] # Test dataset test = df[t:] print(&quot;number of test samples :&quot;, test.shape[0]) print(&quot;number of training samples:&quot;,train.shape[0]) . number of test samples : 103 number of training samples: 925 . Test for stationarity and finding the &#8220;d&#8221; value: . The primary purpose of differencing in the ARIMA model is to make the Time Series stationary. But if the time series is already stationary then we would use the ARMA model instead because the “d” value will be 0. . A stationary time series means a time series without a trend, one having a constant mean and variance over time,a stationary series (also called a “white noise process”) is easier to analyse as it can be modelled with fewer parameters. While it may fluctuate, it will always revert to a constant mean and is thus easier to predict. . NOTE: A time series is considered strictly stationary if the probability distribution of a sequence of observations is unchanged by shifts in time. Strictly stationary series are rare, and it is often enough to assume weak stationarity. . but how would we know whether our data is stationary or not? . We can check for stationarity visually and statistically . Visual test: . train[&#39;rolmean&#39;] = train[&#39;Close&#39;].rolling(20).mean() train[&#39;rolstd&#39;] = train[&#39;Close&#39;].rolling(20).std() . plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(12, 6)) orig = plt.plot(df_copy.index[:925], train[&#39;Close&#39;], color=&#39;red&#39;) plt.title(&#39;Price&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:13.206499 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(12, 6)) mean = plt.plot(df_copy.index[:925],train[&#39;rolmean&#39;], color=&#39;red&#39;) plt.title(&#39;Rolling Mean&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:14.318328 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(12, 6)) std = plt.plot(df_copy.index[:925],train[&#39;rolstd&#39;], color=&#39;red&#39;) plt.title(&#39;Rolling Standard Deviation&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:15.328373 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ Visually we can see clearly that PG has been trending higher for the past 5 years, it’s been making higher highs and higher lows, and on a monthly bases, the mean is also trending higher and the standard deviation is also not constant and fluctuates over time . Statistical test: . To check for stationary statistically we use the Augmented Dickey-Fuller(ADF) which is more accurate than making a visual observation. . In statistics and econometrics, an augmented Dickey–Fuller test is used to test whether a given time series is stationary or not. . The p-value resulting from the ADF test has to be less than 0.05 or 5% for a time series to be stationary. If the p-value is greater than 0.05 or 5%, you conclude that the time series has a unit root which means that it is a non-stationary process. . result = adfuller(df[&#39;Close&#39;]) print(&#39;Augmented Dickey-Fuller Test:&#39;) labels = [&#39;ADF Test Statistic&#39;,&#39;p-value&#39;,&#39;#Lags Used&#39;,&#39;Number of Observations Used&#39;] for value,label in zip(result,labels): print(label+&#39; : &#39;+str(value) ) if result[1] &lt;= 0.05: print(&quot;strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary&quot;) else: print(&quot;weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary &quot;) . Augmented Dickey-Fuller Test: ADF Test Statistic : -1.7582424618957933 p-value : 0.4013812122884916 #Lags Used : 9 Number of Observations Used : 1018 weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary . We have made a mathematical and empirical observation that our time series is not stationary hence we rule out the idea of using an ARMA model because we in fact do need to difference our time series inorder to achieve (weak) stationarity . Differencing: . Differencing is a method of making a times series dataset stationary, by subtracting the observation in the previous time step from the current observation. This process can be repeated more than once, and the number of times differencing is performed is called the difference order. . Let’s look at the equation again: . $Y_{t}^&#8242; = Y_{t} &#8722; Y_{t&#8722;1}$ . So now the question is at what difference order? Once, twice?. The aim is to avoid over-differencing because it can lead to loss of valuable information about the time series and this often affects the construction of the model . We could repeat the visual and statistical test every time we difference our time series but fortunately, there is a helpful package we can use called pmdarima and it will tell us at what order should we difference our time series (under the hood it repeats our statistical test and constantly checks at what order will the null hypothesis be rejected) . ndiffs(train[&#39;Close&#39;], test=&quot;adf&quot;) . 1 . So in order to make the series stationary we only need to difference it once! . diff = train[&#39;Close&#39;].diff().dropna() diff_rolmean = diff.rolling(20).mean() diff_rolstd = diff.rolling(20).std() plt.figure(figsize=(12, 6)) plt.plot(df_copy.index[:924],diff, color=&#39;red&#39;, label=&#39;Original&#39;) plt.plot(diff_rolmean, color=&#39;green&#39;, label=&#39;Rolling Mean&#39;) plt.plot(diff_rolstd, color=&#39;blue&#39;, label = &#39;Rolling Std Deviation&#39;) plt.title(&#39;1st order difference&#39;) plt.legend(loc=&#39;best&#39;) plt.grid(False) result2 = adfuller(diff) print(&#39;Augmented Dickey-Fuller Test:&#39;) for value,label in zip(result2,labels): print(label+&#39; : &#39;+str(value) ) if result2[1] &lt;= 0.05: print(&quot;strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary&quot;) else: print(&quot;weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary &quot;) . Augmented Dickey-Fuller Test: ADF Test Statistic : -10.202850337835578 p-value : 5.90798356244438e-18 #Lags Used : 8 Number of Observations Used : 915 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:23.780819 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ Note: after differencing we can see it does look stationary but it is obvious that there are huge fluctuations in mid-2020 this was due to volatility being extremely high, in fact, On March 16, 2020, the VIX closed at a record high of 82.69 as investors and traders reacted to the pandemic. . So now we have the “d” value in the ARIMA which is 1 . ARIMA(p,1,q) . Finding the &quot;p&quot; and &quot;q&quot; value using PACF and ACF: . The next step is to determine the appropriate order of AR (p) and MA(q) processes by using the Partial Autocorrelation function (PACF)and Autocorrelation function (ACF) . PACF for AR(p): . The order, p, of the autoregressive model can be determined by looking at the partial autocorrelation function (PACF) plot. Partial autocorrelation can be imagined as the correlation between the series and its lag, after excluding the contributions from the intermediate lags. In other words, partial autocorrelation is the relation between observed at two-time spots given that we consider both observations are correlated to the observations at other time spots. For example, today’s stock price can be correlated to the day before yesterday, and yesterday can also be correlated to the day before yesterday… day 1 with day 3, day 2 with day 3 . ACF for MA(q): . The order q can be determined by the Autocorrelation Function plot (ACF) which checks for correlation between two different data points of a Time Series separated by a lag “h”. For example, for lag 1, today’s stock price can be correlated with yesterday’s and yesterday&#39;s stock price and can be correlated with the day before yesterday… day 1 with day 2, day 2 with day 3, etc, etc . plt.style.use(&#39;dark_background&#39;) #PACF to find p fig, ax = plt.subplots(figsize=(12,5)) plot_pacf(diff, lags=20, ax=ax,color=&#39;red&#39;) plt.grid(False) #ACF to find q fig, ax = plt.subplots(figsize=(12,5)) plot_acf(diff, lags=20, ax=ax, color=&#39;red&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:27.593500 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:28.455093 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ PACF and ACF Plot: . The light/shaded area shows the significant threshold value, and every vertical line indicates the PACF and ACF values at each time spot. So in the plot, only the vertical lines that exceed the light/shaded red area are considered significant. . For PACF we can see that the PACF lag 1,4,7,9 and 15 are significant, but I will pick lag 7 since it is well above the significance line compared to the rest. So, we will set the AR(p) value equal to 7. . While for the ACF, the lags 1,4,7 and 9 are significant, but lag 4 seems to be well above the significance line compared to the other lags. So, we will set the MA(q) value equal to 4. . NOTE: Before we continue something useful to remember is that the ACF can also be used to check for stationarity, for a stationary series, the autocorrelation in the ACF plot should decay quickly; with a non-stationary series, the ACF will decay slowly as we’ve already seen . plt.style.use(&#39;dark_background&#39;) fig, ax = plt.subplots(figsize=(12,3)) plot_acf(train[&#39;Close&#39;], lags=50, ax=ax,color=&#39;red&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:30.718366 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ So we have our “p” and “q” values, which are 7 and 4. . ARIMA(7,1,4) . Model development &amp; Evaluation: . Now that we have determined the parameters (p,d,q), we will estimate the accuracy of the ARIMA model on a training data set and then use the fitted model to forecast the values of the test data set using a forecasting function. In the end, we cross-check whether our forecasted values are in line with the actual values. . model = ARIMA(train[&#39;Close&#39;], order=(7,1,4)) result = model.fit() . residuals = pd.DataFrame({&#39;residuals&#39;:result.resid}) plt.style.use(&#39;dark_background&#39;) residuals[1:].plot(kind=&#39;hist&#39;,bins=50,figsize=(12,5),color=&#39;red&#39;) plt.title(&#39;Residuals&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:37.575322 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ The residuals are the error based of what it would predict for the train data, the closer the residuals are to zero the better . pred = result.get_forecast(&#39;2022-06-29&#39;).summary_frame() pred = pred[&#39;2022-02-01&#39;:] plt.style.use(&#39;dark_background&#39;) pred[&#39;mean&#39;][&#39;2022-02-01&#39;:].plot(figsize=(12,8),color=&#39;black&#39;,label=&#39;Forecast&#39;) test[&#39;Close&#39;].plot(figsize=(12,8),color=&#39;red&#39;,label=&#39;Test Price&#39;) train[&#39;Close&#39;].plot(figsize=(12,8),color=&#39;blue&#39;,label=&#39;Train Price&#39;) #plt.fill_between(pred.index, pred[&#39;mean_ci_lower&#39;], pred[&#39;mean_ci_upper&#39;], color=&#39;k&#39;, alpha=0.1); plt.fill_between(pred.index, pred[&#39;mean_ci_lower&#39;], pred[&#39;mean_ci_upper&#39;], color=&#39;gray&#39;) plt.title(&#39;ARIMA Price Forecast and Confidence Interval&#39;) plt.legend(loc=&#39;best&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:41:39.607425 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ pred[&#39;mean&#39;][&#39;2022-02-01&#39;:] . 2022-02-01 159.739671 2022-02-02 159.739671 2022-02-03 159.739671 2022-02-04 159.739671 2022-02-05 159.739671 ... 2022-06-25 159.739671 2022-06-26 159.739671 2022-06-27 159.739671 2022-06-28 159.739671 2022-06-29 159.739671 Freq: D, Name: mean, Length: 149, dtype: float64 . The values seem to be flat, hence the prediction is far from accurate at the very least I was expecting the model to predict a trend in either direction . A possible solution to get a more accurate result using the ARIMA would be to test with different lags, remember we got several lags that we could’ve used, we can test a combination of all of them to find the one which can offer better result . But obviously, this will take time, a solution to that is to use the same package that we used to get the number of differencing needed to make our time series stationary. . AUTO ARIMA: . We can implement the Auto ARIMA model using the pmdarima time-series library which provides the auto_arima() function that automatically generates the optimal parameter values for all 3 p,d,q values . The auto_arima() function can take a number of paramters but to keep it simple i&#39;ll just run the function and get the default best model, all we need to do is just pass in our time series data. . from pmdarima.arima import auto_arima auto_arima = auto_arima(train[&quot;Close&quot;]) auto_arima . ARIMA(order=(4, 1, 5), scoring_args={}, suppress_warnings=True) . print(f&#39;The default best value is {auto_arima.fit(train[&quot;Close&quot;])}&#39;) . The default best value is ARIMA(4,1,5)(0,0,0)[0] intercept . model2 = ARIMA(train[&#39;Close&#39;], order=(4,1,5)) result2 = model2.fit() . residuals2 = pd.DataFrame({&#39;residuals&#39;:result2.resid}) plt.style.use(&#39;dark_background&#39;) residuals2[1:].plot(kind=&#39;hist&#39;,bins=50,figsize=(12,5),color=&#39;red&#39;) plt.title(&#39;Residuals&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:42:25.441546 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ pred2 = result2.get_forecast(&#39;2022-06-29&#39;).summary_frame() pred2 = pred2[&#39;2022-02-01&#39;:] plt.style.use(&#39;dark_background&#39;) pred2[&#39;mean&#39;][&#39;2022-02-01&#39;:].plot(figsize=(12,8),color=&#39;black&#39;,label=&#39;Forecast&#39;) test[&#39;Close&#39;].plot(figsize=(12,8),color=&#39;red&#39;,label=&#39;Test Price&#39;) train[&#39;Close&#39;].plot(figsize=(12,8),color=&#39;blue&#39;,label=&#39;Train Price&#39;) plt.fill_between(pred2.index, pred2[&#39;mean_ci_lower&#39;], pred2[&#39;mean_ci_upper&#39;], color=&#39;gray&#39;) plt.title(&#39;ARIMA Price Forecast and Confidence Interval&#39;) plt.legend(loc=&#39;best&#39;) plt.grid(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-07-20T17:42:26.239612 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ pred2[&#39;mean&#39;][&#39;2022-02-01&#39;:] . 2022-02-01 159.472243 2022-02-02 159.472243 2022-02-03 159.472243 2022-02-04 159.472243 2022-02-05 159.472243 ... 2022-06-25 159.472243 2022-06-26 159.472243 2022-06-27 159.472243 2022-06-28 159.472243 2022-06-29 159.472243 Freq: D, Name: mean, Length: 149, dtype: float64 . Conclusion: . Even after the default best model, the values haven&#39;t changed much as they look constant from afar. . Stock data is complex and are usually composed of linear and non-linear components. So just using a linear model to make predictions will not be efficient, especially using ARIMA to predict prices as we saw didn’t provide a realistic prediction, they may however be useful to predict the overall trend of a time series. . Alternatively, we could try and predict stock returns instead because returns are considered stationary(at least visually) or we could use the SARIMA (Seasonal Auto-Regressive Integrated Moving Average) model which is an extension of the ARIMA, it adds seasonal components of the time series that can help improve prediction. . But again something to remember is that even if we continue to fine-tune the ARIMA and SARIMA models, the prediction could still be inaccurate because we will still be using univariate data, in order to get the best predictions it’s better to use multivariate data like using the closing prices or stock return of other correlated consumer staple stocks like Colgate-Palmolive Company (TICKER: CL) or The Clorox Company (TICKER: CLX) using models like the ARIMAX or SARIMAX which are just an extension of the ARIMA and SARIMA models, The ‘X’ stands for an exogenous variable(or variables) and in this example, CL and CLX could be those exogenous variables. . For now, my knowledge is still limited, more time is needed to research and learn about other models as well learning in-depth analysis and forecasting techniques, hence, In future blogs, I will try and incorporate all relevant models and leverage the pmdarima package even further by adding extra parameters to try to find the most accurate model .",
            "url": "https://mjabubakar22.github.io/Monewmetrics/quantitative%20research/2022/07/03/ARIMA-prediction.html",
            "relUrl": "/quantitative%20research/2022/07/03/ARIMA-prediction.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "BTC/USD Price Prediction Using Linear Regression",
            "content": "DISCLAIMER! . Before proceeding, please make sure that you note the following important information: . NOT FINANCIAL ADVICE! . My content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice, None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument to make any investment or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur . import pandas as pd import numpy as np import yfinance as yf from scipy import stats from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error, mean_absolute_error from datetime import datetime import plotly.express as px import plotly.graph_objs as go from plotly.offline import init_notebook_mode, iplot init_notebook_mode(connected=True) from IPython.display import HTML import warnings warnings.simplefilter(&quot;ignore&quot;) . Import Data . #BTC price and volume #you will import Gold price when you start creating the indipendant variables # Read data Df = yf.download(&#39;BTC-USD&#39;, &#39;2012-01-01&#39;, &#39;2022-05-31&#39;, interval= &#39;1mo&#39;, auto_adjust=True) # Only keep close columns Df = Df[[&#39;Close&#39;,&#39;Volume&#39;]] # Drop rows with missing values Df = Df.dropna() . [*********************100%***********************] 1 of 1 completed . Data Cleaning and Wrangling . Before any modeling can be done, there are a few steps needed to prepare the data before feeding it to the model, at least by arranging the data set in a way it makes sense . df = Df.reset_index() for i in [&#39;Close&#39;, &#39;Volume&#39;]: df[i] = df[i].astype(&#39;float64&#39;) df . Date Close Volume . 0 2014-10-01 | 338.321014 | 9.029944e+08 | . 1 2014-11-01 | 378.046997 | 6.597334e+08 | . 2 2014-12-01 | 320.192993 | 5.531023e+08 | . 3 2015-01-01 | 217.464005 | 1.098812e+09 | . 4 2015-02-01 | 254.263000 | 7.115187e+08 | . ... ... | ... | ... | . 87 2022-01-01 | 38483.125000 | 9.239790e+11 | . 88 2022-02-01 | 43193.234375 | 6.713360e+11 | . 89 2022-03-01 | 45538.675781 | 8.309438e+11 | . 90 2022-04-01 | 37714.875000 | 8.301159e+11 | . 91 2022-05-01 | 31792.310547 | 1.105689e+12 | . 92 rows × 3 columns . Because I&#39;m working with monthly data, I&#39;ll drop the days in the date to avoid confusion . date_format = &quot;%Y/%m&quot; df[&#39;Date&#39;] = df[&#39;Date&#39;].dt.strftime(date_format) df . Date Close Volume . 0 2014/10 | 338.321014 | 9.029944e+08 | . 1 2014/11 | 378.046997 | 6.597334e+08 | . 2 2014/12 | 320.192993 | 5.531023e+08 | . 3 2015/01 | 217.464005 | 1.098812e+09 | . 4 2015/02 | 254.263000 | 7.115187e+08 | . ... ... | ... | ... | . 87 2022/01 | 38483.125000 | 9.239790e+11 | . 88 2022/02 | 43193.234375 | 6.713360e+11 | . 89 2022/03 | 45538.675781 | 8.309438e+11 | . 90 2022/04 | 37714.875000 | 8.301159e+11 | . 91 2022/05 | 31792.310547 | 1.105689e+12 | . 92 rows × 3 columns . Bitcoin is often referred to as &quot;digital gold&quot; by its backers hence I&#39;ll add the gold price and volume data as potential independent variables and I&#39;ll explore further to see its relationship and whether or not it will be a good predictor . gold = yf.download(&#39;GLD&#39;, &#39;2014-10-01&#39;, &#39;2022-05-31&#39;, interval= &#39;1mo&#39;, auto_adjust=True) gold = gold[[&#39;Close&#39;,&#39;Volume&#39;]] gld = gold.reset_index() for i in [&#39;Close&#39;, &#39;Volume&#39;]: gld[i] = gld[i].astype(&#39;float64&#39;) gld . [*********************100%***********************] 1 of 1 completed . Date Close Volume . 0 2014-10-01 | 112.660004 | 155183900.0 | . 1 2014-11-01 | 112.110001 | 147594200.0 | . 2 2014-12-01 | 113.580002 | 153722200.0 | . 3 2015-01-01 | 123.449997 | 198034100.0 | . 4 2015-02-01 | 116.160004 | 125686200.0 | . ... ... | ... | ... | . 87 2022-01-01 | 168.089996 | 211125100.0 | . 88 2022-02-01 | 178.380005 | 254601300.0 | . 89 2022-03-01 | 180.649994 | 377087100.0 | . 90 2022-04-01 | 176.910004 | 195346400.0 | . 91 2022-05-01 | 171.139999 | 179902200.0 | . 92 rows × 3 columns . The other two independent variables will be the moving averages and volume . moving averages are often used by technical analysts to keep track of price trends for specific securities. I&#39;ll use the 3 and 6 month exponential moving averages but whether it&#39;s simple, weighted, or exponential in general it doesn&#39;t really make much of a difference (but this could be a good hypothesis to test) . Volume is also a well-known indicator of price movement, Trading volume is the total number of shares/units of a security traded during a given period of time. . df[&#39;ema3&#39;] = df[&#39;Close&#39;].ewm(span=3, adjust=False).mean() df[&#39;ema6&#39;] = df[&#39;Close&#39;].ewm(span=6, adjust=False).mean() df . Date Close Volume ema3 ema6 . 0 2014/10 | 338.321014 | 9.029944e+08 | 338.321014 | 338.321014 | . 1 2014/11 | 378.046997 | 6.597334e+08 | 358.184006 | 349.671295 | . 2 2014/12 | 320.192993 | 5.531023e+08 | 339.188499 | 341.248923 | . 3 2015/01 | 217.464005 | 1.098812e+09 | 278.326252 | 305.881804 | . 4 2015/02 | 254.263000 | 7.115187e+08 | 266.294626 | 291.133574 | . ... ... | ... | ... | ... | ... | . 87 2022/01 | 38483.125000 | 9.239790e+11 | 44519.703176 | 46247.967191 | . 88 2022/02 | 43193.234375 | 6.713360e+11 | 43856.468776 | 45375.186387 | . 89 2022/03 | 45538.675781 | 8.309438e+11 | 44697.572278 | 45421.897642 | . 90 2022/04 | 37714.875000 | 8.301159e+11 | 41206.223639 | 43219.891173 | . 91 2022/05 | 31792.310547 | 1.105689e+12 | 36499.267093 | 39954.868137 | . 92 rows × 5 columns . df[&#39;Gold Close&#39;] = gld[&#39;Close&#39;] df[&#39;Gold Volume&#39;] = gld[&#39;Volume&#39;] df . Date Close Volume ema3 ema6 Gold Close Gold Volume . 0 2014/10 | 338.321014 | 9.029944e+08 | 338.321014 | 338.321014 | 112.660004 | 155183900.0 | . 1 2014/11 | 378.046997 | 6.597334e+08 | 358.184006 | 349.671295 | 112.110001 | 147594200.0 | . 2 2014/12 | 320.192993 | 5.531023e+08 | 339.188499 | 341.248923 | 113.580002 | 153722200.0 | . 3 2015/01 | 217.464005 | 1.098812e+09 | 278.326252 | 305.881804 | 123.449997 | 198034100.0 | . 4 2015/02 | 254.263000 | 7.115187e+08 | 266.294626 | 291.133574 | 116.160004 | 125686200.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 87 2022/01 | 38483.125000 | 9.239790e+11 | 44519.703176 | 46247.967191 | 168.089996 | 211125100.0 | . 88 2022/02 | 43193.234375 | 6.713360e+11 | 43856.468776 | 45375.186387 | 178.380005 | 254601300.0 | . 89 2022/03 | 45538.675781 | 8.309438e+11 | 44697.572278 | 45421.897642 | 180.649994 | 377087100.0 | . 90 2022/04 | 37714.875000 | 8.301159e+11 | 41206.223639 | 43219.891173 | 176.910004 | 195346400.0 | . 91 2022/05 | 31792.310547 | 1.105689e+12 | 36499.267093 | 39954.868137 | 171.139999 | 179902200.0 | . 92 rows × 7 columns . Now i&#39;m going to generate the dependant/target variable that i&#39;m going to try and predict . df[&#39;Next Month Close&#39;] = df[&#39;Close&#39;].shift(-1) df . Date Close Volume ema3 ema6 Gold Close Gold Volume Next Month Close . 0 2014/10 | 338.321014 | 9.029944e+08 | 338.321014 | 338.321014 | 112.660004 | 155183900.0 | 378.046997 | . 1 2014/11 | 378.046997 | 6.597334e+08 | 358.184006 | 349.671295 | 112.110001 | 147594200.0 | 320.192993 | . 2 2014/12 | 320.192993 | 5.531023e+08 | 339.188499 | 341.248923 | 113.580002 | 153722200.0 | 217.464005 | . 3 2015/01 | 217.464005 | 1.098812e+09 | 278.326252 | 305.881804 | 123.449997 | 198034100.0 | 254.263000 | . 4 2015/02 | 254.263000 | 7.115187e+08 | 266.294626 | 291.133574 | 116.160004 | 125686200.0 | 244.223999 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 87 2022/01 | 38483.125000 | 9.239790e+11 | 44519.703176 | 46247.967191 | 168.089996 | 211125100.0 | 43193.234375 | . 88 2022/02 | 43193.234375 | 6.713360e+11 | 43856.468776 | 45375.186387 | 178.380005 | 254601300.0 | 45538.675781 | . 89 2022/03 | 45538.675781 | 8.309438e+11 | 44697.572278 | 45421.897642 | 180.649994 | 377087100.0 | 37714.875000 | . 90 2022/04 | 37714.875000 | 8.301159e+11 | 41206.223639 | 43219.891173 | 176.910004 | 195346400.0 | 31792.310547 | . 91 2022/05 | 31792.310547 | 1.105689e+12 | 36499.267093 | 39954.868137 | 171.139999 | 179902200.0 | NaN | . 92 rows × 8 columns . btc_close = df[&#39;Close&#39;] #But i&#39;ll save the close price just incase i need it #Then i&#39;ll remove the previous month btc close price so that i&#39;m left with only the relevant data that i need df.drop(columns=&#39;Close&#39;, inplace=True) . Df = df.dropna() #Now i should have a good clean dataframe ready for some EDA Df . Date Volume ema3 ema6 Gold Close Gold Volume Next Month Close . 0 2014/10 | 9.029944e+08 | 338.321014 | 338.321014 | 112.660004 | 155183900.0 | 378.046997 | . 1 2014/11 | 6.597334e+08 | 358.184006 | 349.671295 | 112.110001 | 147594200.0 | 320.192993 | . 2 2014/12 | 5.531023e+08 | 339.188499 | 341.248923 | 113.580002 | 153722200.0 | 217.464005 | . 3 2015/01 | 1.098812e+09 | 278.326252 | 305.881804 | 123.449997 | 198034100.0 | 254.263000 | . 4 2015/02 | 7.115187e+08 | 266.294626 | 291.133574 | 116.160004 | 125686200.0 | 244.223999 | . ... ... | ... | ... | ... | ... | ... | ... | . 86 2021/12 | 9.570472e+11 | 50556.281353 | 49353.904068 | 170.960007 | 151214100.0 | 38483.125000 | . 87 2022/01 | 9.239790e+11 | 44519.703176 | 46247.967191 | 168.089996 | 211125100.0 | 43193.234375 | . 88 2022/02 | 6.713360e+11 | 43856.468776 | 45375.186387 | 178.380005 | 254601300.0 | 45538.675781 | . 89 2022/03 | 8.309438e+11 | 44697.572278 | 45421.897642 | 180.649994 | 377087100.0 | 37714.875000 | . 90 2022/04 | 8.301159e+11 | 41206.223639 | 43219.891173 | 176.910004 | 195346400.0 | 31792.310547 | . 91 rows × 7 columns . Explanatory Data Analysis . Intuitley I know that traders like to use the ema lines and volume to predict BTC price. But as I mentioned before here I get the chance to explore whether gold price and its volume can help predict BTC price. . The Pearson correlation coefficient and p value . Pearson Correlation: . Correlation between sets of data is a measure of how well they are related. The most common measure of correlation in stats is the Pearson Correlation.The full name is the Pearson Product Moment Correlation (PPMC). It shows the linear relationship between two sets of data. In simple terms, it answers the question, Can I draw a line graph to represent the data? . It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables, where: 1: Perfect positive linear correlation. | 0: No linear correlation, the two variables most likely do not affect each other. | -1: Perfect negative linear correlation. | . P-Value: . A p-value measures the probability of obtaining the observed results, assuming that the null hypothesis is true. The lower the p-value, the greater the statistical significance of the observed difference. A p-value of 0.05 or lower is generally considered statistically significant which means that we are 95% confident that the correlation between the variables is significant. . By convention, when the . p-value is &lt; 0.001: we say there is strong evidence that the correlation is significant. | the p-value is &lt; 0.05: there is moderate evidence that the correlation is significant. | the p-value is &lt; 0.1: there is weak evidence that the correlation is significant. | the p-value is &gt; 0.1: there is no evidence that the correlation is significant. | . Two things keeps to keep in mind when interprating the results: . The null hypothesis is that the two variables are uncorrelated . | The p value is in scientific notation, it&#39;s decimal form is e.g 4.2e-7 = 0.00000042. | . #I will start form the second last row to avoid errors bcz of nan value pearson_coef, p_value = stats.pearsonr(df[&#39;Gold Close&#39;][:90], df[&#39;Next Month Close&#39;][:90]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) . The Pearson Correlation Coefficient is 0.7706711768570538 with a P-value of P = 6.555477090301474e-19 . In this case, . The p-value is &lt; 0.001 hence, there is strong evidence that the correlation between gold price and BTC price is statistically significant, and the linear relationship is quite strong (0.77, close to 1) . pearson_coef, p_value = stats.pearsonr(df[&#39;Gold Volume&#39;][:90], df[&#39;Next Month Close&#39;][:90]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) . The Pearson Correlation Coefficient is 0.08476385450593574 with a P-value of P = 0.42700282567060693 . The p-value is &lt; 0.001 hence, there is moderate evidence that the correlation between gold volume and BTC price is statistically significant, and there is no linear relationship (0.08, almost 0) . Visually we can see that there is almost no linear relationship between gold volume and btc price . fig = px.scatter( df, x=&#39;Gold Volume&#39;, y=&#39;Next Month Close&#39;, opacity=0.65, trendline=&#39;ols&#39;, trendline_color_override=&#39;firebrick&#39; ) fig.update_layout(template = &quot;plotly_dark&quot;) fig.show() . So we now know that we can use gold price but not its volume, it would have destroyed value and it wouldn&#39;t have added anything to the model if anything it would have probably ruined our prediction . df.drop(columns=&#39;Gold Volume&#39;, inplace=True) df.head() . Date Volume ema3 ema6 Gold Close Next Month Close . 0 2014/10 | 9.029944e+08 | 338.321014 | 338.321014 | 112.660004 | 378.046997 | . 1 2014/11 | 6.597334e+08 | 358.184006 | 349.671295 | 112.110001 | 320.192993 | . 2 2014/12 | 5.531023e+08 | 339.188499 | 341.248923 | 113.580002 | 217.464005 | . 3 2015/01 | 1.098812e+09 | 278.326252 | 305.881804 | 123.449997 | 254.263000 | . 4 2015/02 | 7.115187e+08 | 266.294626 | 291.133574 | 116.160004 | 244.223999 | . What about the the other indipendant variables? . print(&#39;EMA 3&#39;) pearson_coef, p_value = stats.pearsonr(df[&#39;ema3&#39;][:90], df[&#39;Next Month Close&#39;][:90]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) print(&#39; nEMA 6&#39;) pearson_coef, p_value = stats.pearsonr(df[&#39;ema6&#39;][:90], df[&#39;Next Month Close&#39;][:90]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) print(&#39; nVolume&#39;) pearson_coef, p_value = stats.pearsonr(df[&#39;Volume&#39;][:90], df[&#39;Next Month Close&#39;][:90]) print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) . EMA 3 The Pearson Correlation Coefficient is 0.9518278081533298 with a P-value of P = 5.913915911148407e-47 EMA 6 The Pearson Correlation Coefficient is 0.9357719658423489 with a P-value of P = 1.3124438502636034e-41 Volume The Pearson Correlation Coefficient is 0.7995337431531997 with a P-value of P = 3.465784507142274e-21 . The other indipendant variables are all statistically significant, and their linear relationship are very strong with p-values of &lt; 0.001 . NOTE: . Causation is the relationship between cause and effect between two variables. . It is important to know the difference between correlation and causation. Correlation does not imply causation. Determining correlation is much simpler than determining causation as causation may require independent experimentation. . Model Development . Before we continue let&#39;s clarify the objective again: I&#39;m using the 3 and 6 month ema, BTC previous month volume, and Gold close price of the preceding month to predict what BTC close price of the impending month . x = Df[[&#39;ema3&#39;,&#39;ema6&#39;,&#39;Volume&#39;,&#39;Gold Close&#39;]] # Define the dependent variable y = Df[&#39;Next Month Close&#39;] . I am going to spilt the data, 80% of the data will be used to train the model and 20% will be used to test the prediction made from that 80% . t = .8 t = int(t*len(Df)) # Train dataset x_train = x[:t] y_train = y[:t] # Test dataset x_test = x[t:] y_test = y[t:] . print(&quot;number of test samples :&quot;, y_test.shape[0]) print(&quot;number of training samples:&quot;,y_train.shape[0]) . number of test samples : 19 number of training samples: 72 . reg = linear_model.LinearRegression() reg.fit(x_train,y_train) . LinearRegression() . The constant came back negative which is confusing but i&#39;ll get back to this later . reg.intercept_ . -1262.8899087871669 . coeff_df = pd.DataFrame(reg.coef_.T, x.columns, columns=[&#39;Coefficient&#39;]) coeff_df . Coefficient . ema3 1.777304e+00 | . ema6 -9.488261e-01 | . Volume 9.555041e-10 | . Gold Close 1.521999e+01 | . The Multiple linear regression formula: . $$ y = &#946;_{0} ;+ ;&#946;_{1} * ;X_{1} + ;&#946;_{2} * ;X_{2} + ;&#946;_{3} * ;X_{3} + ;&#946;_{4} * ;X_{4}$$ . print(&quot;Linear Regression model&quot;) print(&quot;BTC Price (y) = %.2f (constant) + %.2f * EM3 (x1) + %.2f * EMA6 (x2) + %.4f * Volume (x3) + %.2f * Gold Close (x4)&quot; % (reg.intercept_,reg.coef_[0], reg.coef_[1],reg.coef_[2],reg.coef_[3])) . Linear Regression model BTC Price (y) = -1262.89 (constant) + 1.78 * EM3 (x1) + -0.95 * EMA6 (x2) + 0.0000 * Volume (x3) + 15.22 * Gold Close (x4) . Model Evaluation . In this step, I will evaluate the model&#39;s accuracy but before that happens I&#39;m going to make the predictions first . predicted_price = reg.predict(x_test) . The R square is 0.05 which means the model’s predicitive power is poor in fact it is worse than what I expected it predicts little to nothing of the target variable . test_r2_score = r2_score(y_test,predicted_price) print(&#39;The test R-square is: &#39;, test_r2_score) . The test R-square is: 0.053315838215583056 . I will also look at other evaluation methods . test_r2_score = r2_score(y_test,predicted_price) print(&#39;The test R-square is: &#39;, test_r2_score) test_mse = mean_squared_error(y_test, predicted_price) print(&#39;The test mean square error of target variable and predicted value is: &#39;, test_mse) test_mae = mean_absolute_error(y_test, predicted_price) print(&#39;The test mean absolute error of target variable and predicted value is: &#39;, test_mae) test_rmse=np.sqrt(test_mse) print(&#39;The test root mean square error of target variable and predicted value is: &#39;, test_rmse) . The test R-square is: 0.053315838215583056 The test mean square error of target variable and predicted value is: 107191024.05803142 The test mean absolute error of target variable and predicted value is: 8821.373009960726 The test root mean square error of target variable and predicted value is: 10353.309811747711 . Mean Square Error (MSE) is the average difference of actual values and predicted values There is no correct value for MSE. Simply put, the lower the value the better, and 0 means the model is perfect. . Mean Absolute Error (MAE) is the sum of the absolute difference between actual and predicted values in this case the average difference is $8821 . I&#39;m going to evaluate further and try to see what other insights I can gather from the predicted price I&#39;ll start by creating a data frame and add the predicted price and actual price so that I can plot the prices side by side . btc = pd.DataFrame() #btc[&#39;Close Previous Month&#39;] = btc_close[t:] btc[&#39;Date&#39;] = Df[&#39;Date&#39;][t:] btc[&#39;Predicted Close&#39;] = predicted_price btc[&#39;Actual Close&#39;] = btc_close[t:].shift(-1)#btc[&#39;Close Previous Month&#39;].shift(-1) btc . Date Predicted Close Actual Close . 72 2020/10 | 13607.631908 | 19625.835938 | . 73 2020/11 | 17737.624104 | 29001.720703 | . 74 2020/12 | 25451.933856 | 33114.359375 | . 75 2021/01 | 31623.812022 | 45137.769531 | . 76 2021/02 | 40797.895059 | 58918.832031 | . 77 2021/03 | 52011.667637 | 57750.175781 | . 78 2021/04 | 55687.583652 | 37332.855469 | . 79 2021/05 | 43935.783612 | 35040.835938 | . 80 2021/06 | 35882.237824 | 41626.195312 | . 81 2021/07 | 36483.673993 | 47166.687500 | . 82 2021/08 | 40423.918840 | 43790.894531 | . 83 2021/09 | 39815.250546 | 61318.957031 | . 84 2021/10 | 50330.042450 | 57005.425781 | . 85 2021/11 | 51684.668062 | 46306.445312 | . 86 2021/12 | 45279.181353 | 38483.125000 | . 87 2022/01 | 37422.063673 | 43193.234375 | . 88 2022/02 | 36986.624235 | 45538.675781 | . 89 2022/03 | 38624.254997 | 37714.875000 | . 90 2022/04 | 34450.675253 | 31792.310547 | . fig = px.line(btc, x=&quot;Date&quot;, y=btc.columns, title=&#39;Predicted Close Vs Actual Close&#39;) fig.update_xaxes( rangeslider_visible=True, rangeselector=dict( buttons=list([ dict(count=1, label=&quot;1m&quot;, step=&quot;month&quot;, stepmode=&quot;backward&quot;), dict(count=6, label=&quot;6m&quot;, step=&quot;month&quot;, stepmode=&quot;backward&quot;), dict(count=1, label=&quot;YTD&quot;, step=&quot;year&quot;, stepmode=&quot;todate&quot;), dict(count=1, label=&quot;1y&quot;, step=&quot;year&quot;, stepmode=&quot;backward&quot;), dict(step=&quot;all&quot;) ]) ) ) fig.update_layout(template = &quot;plotly_dark&quot;) . We have already seen the mean of these residuals (mean squared error), now I&#39;ll look at the residuals of each month in absolute and in % to see how far off are the predictions for each month (Remember, the residual is the difference between the observed value and the estimated value) . btc[&#39;Residual&#39;] = btc_close[t:] - btc[&#39;Actual Close&#39;] #The difference in absolute $ terms btc[&#39;Residual in %&#39;] = np.absolute(btc[&#39;Residual&#39;]/btc[&#39;Actual Close&#39;]*100) #The difference in % btc . Date Predicted Close Actual Close Residual Residual in % . 72 2020/10 | 13607.631908 | 19625.835938 | -5844.840820 | 29.781360 | . 73 2020/11 | 17737.624104 | 29001.720703 | -9375.884766 | 32.328719 | . 74 2020/12 | 25451.933856 | 33114.359375 | -4112.638672 | 12.419502 | . 75 2021/01 | 31623.812022 | 45137.769531 | -12023.410156 | 26.637138 | . 76 2021/02 | 40797.895059 | 58918.832031 | -13781.062500 | 23.389911 | . 77 2021/03 | 52011.667637 | 57750.175781 | 1168.656250 | 2.023641 | . 78 2021/04 | 55687.583652 | 37332.855469 | 20417.320312 | 54.689951 | . 79 2021/05 | 43935.783612 | 35040.835938 | 2292.019531 | 6.540996 | . 80 2021/06 | 35882.237824 | 41626.195312 | -6585.359375 | 15.820229 | . 81 2021/07 | 36483.673993 | 47166.687500 | -5540.492188 | 11.746621 | . 82 2021/08 | 40423.918840 | 43790.894531 | 3375.792969 | 7.708892 | . 83 2021/09 | 39815.250546 | 61318.957031 | -17528.062500 | 28.585063 | . 84 2021/10 | 50330.042450 | 57005.425781 | 4313.531250 | 7.566878 | . 85 2021/11 | 51684.668062 | 46306.445312 | 10698.980469 | 23.104733 | . 86 2021/12 | 45279.181353 | 38483.125000 | 7823.320312 | 20.329223 | . 87 2022/01 | 37422.063673 | 43193.234375 | -4710.109375 | 10.904739 | . 88 2022/02 | 36986.624235 | 45538.675781 | -2345.441406 | 5.150438 | . 89 2022/03 | 38624.254997 | 37714.875000 | 7823.800781 | 20.744602 | . 90 2022/04 | 34450.675253 | 31792.310547 | 5922.564453 | 18.628921 | . As you can see from the residual, the difference is pretty large but this is due to BTC being very volatile hence anything between 7-10% difference (this is subjective based on my observations from BTC trading) could be good but to expect a residual of less than 5% consistently would be very unlikely from an asset class this volatile . Conclusion . If the model had been at least 50-60% accurate (have an R square of 0.50-0.60), I would have proceeded with backtesting and then take the model live by predicting the close price of this month (June 2022) . The linear regression is not a good model to use when predicting BTC/USD prices, maybe it would&#39;ve been more efficient in predicting the returns instead. There were many red flags and based on the R square alone I would never take this model live and risk real money on it . The MSE was way too high and very far from 0 since MSE is a measure of how close a fitted line is to data points . Another red flag was the constant being negative, This means when the independent variables are 0 the mean price of BTC will be -1262. A negative constant doesn&#39;t mean the regression made a mistake but rather it&#39;s the data being modeled, realistically any security price can go to 0 but no security price can fall below 0 and turn negative, The your position value of that asset can turn negative but not the actual asset price. which is also why I think predicting returns instead of price would have been more accurate and much more realistic . This is also a good example to showcase how a machine learning model is only as useful as the features selected and in-order to select the right features depends on the knowledge one has of that data set! .",
            "url": "https://mjabubakar22.github.io/Monewmetrics/quantitative%20research/2022/06/01/btc-usd-price-prediction.html",
            "relUrl": "/quantitative%20research/2022/06/01/btc-usd-price-prediction.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Portfolio Analysis, Efficient Frontier & Monte Carlo",
            "content": "DISCLAIMER! . Before proceeding, please make sure that you note the following important information: . NOT FINANCIAL ADVICE! . My content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument, to make any investment, or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur . import numpy as np import pandas as pd import yfinance as yf from pandas_datareader import data as wb from chart_studio import plotly as py import plotly.express as px import plotly.graph_objs as go from IPython.display import HTML from plotly.offline import init_notebook_mode, iplot init_notebook_mode(connected=True) import matplotlib.pyplot as plt %matplotlib inline plt.style.use(&#39;seaborn-darkgrid&#39;) . Portfolio construction and analysis: . assets = [&#39;TXN&#39;,&#39;CSCO&#39;,&#39;INTC&#39;,&#39;AAPL&#39;,&#39;MSFT&#39;, &#39;NVDA&#39;,&#39;INFY&#39;,&#39;INTU&#39;,&#39;SAP&#39;,&#39;ADI&#39;, &#39;ANSS&#39;,&#39;CRM&#39;,&#39;ADBE&#39;,&#39;FB&#39;,&#39;AMD&#39;, &#39;AMZN&#39;,&#39;MA&#39;,&#39;VMW&#39;,&#39;GOOG&#39;,&#39;SNPS&#39;] pf_data = pd.DataFrame() for a in assets: pf_data[a] = yf.download(a, start=&quot;2012-05-20&quot;, end=&quot;2021-12-31&quot;, index_col = &#39;Date&#39;, parse_dates=True)[&#39;Adj Close&#39;] . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . pf_data.head() . TXN CSCO INTC AAPL MSFT NVDA INFY INTU SAP ADI ANSS CRM ADBE FB AMD AMZN MA VMW GOOG SNPS . Date . 2012-05-21 22.610977 | 12.278630 | 19.480406 | 17.139450 | 24.290533 | 2.821950 | 4.160825 | 50.348274 | 49.764915 | 28.088888 | 61.000000 | 37.262501 | 32.009998 | 34.029999 | 6.30 | 218.110001 | 38.773224 | 69.075218 | 305.908386 | 28.040001 | . 2012-05-22 22.518942 | 12.322823 | 19.391010 | 17.007828 | 24.298697 | 2.787507 | 4.215657 | 51.022617 | 49.680264 | 28.034071 | 61.750000 | 37.362499 | 32.009998 | 31.000000 | 6.16 | 215.330002 | 39.043331 | 68.748131 | 299.278229 | 28.040001 | . 2012-05-23 22.350197 | 12.293363 | 18.951487 | 17.422827 | 23.767975 | 2.856392 | 4.168488 | 51.460045 | 49.773380 | 27.940096 | 62.119999 | 37.652500 | 32.180000 | 32.000000 | 6.08 | 217.279999 | 39.388008 | 69.177422 | 303.592072 | 28.200001 | . 2012-05-24 22.158449 | 12.072392 | 19.107927 | 17.262810 | 23.735321 | 2.780620 | 4.243171 | 51.387135 | 48.842247 | 28.190681 | 61.689999 | 36.552502 | 31.540001 | 33.029999 | 6.02 | 215.240005 | 39.646744 | 65.027603 | 300.702881 | 29.850000 | . 2012-05-25 22.196800 | 12.028200 | 19.174969 | 17.170284 | 23.727154 | 2.847208 | 4.172420 | 51.441814 | 48.308960 | 28.339464 | 62.090000 | 36.750000 | 31.600000 | 31.910000 | 6.22 | 212.889999 | 39.092426 | 64.066803 | 294.660553 | 29.889999 | . pf_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 2420 entries, 2012-05-21 to 2021-12-30 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 TXN 2420 non-null float64 1 CSCO 2420 non-null float64 2 INTC 2420 non-null float64 3 AAPL 2420 non-null float64 4 MSFT 2420 non-null float64 5 NVDA 2420 non-null float64 6 INFY 2420 non-null float64 7 INTU 2420 non-null float64 8 SAP 2420 non-null float64 9 ADI 2420 non-null float64 10 ANSS 2420 non-null float64 11 CRM 2420 non-null float64 12 ADBE 2420 non-null float64 13 FB 2420 non-null float64 14 AMD 2420 non-null float64 15 AMZN 2420 non-null float64 16 MA 2420 non-null float64 17 VMW 2420 non-null float64 18 GOOG 2420 non-null float64 19 SNPS 2420 non-null float64 dtypes: float64(20) memory usage: 397.0 KB . I’ll use a built-in method in DataFrame that computes the percent change from one row to another . returns = pf_data.pct_change(1).dropna() returns . TXN CSCO INTC AAPL MSFT NVDA INFY INTU SAP ADI ANSS CRM ADBE FB AMD AMZN MA VMW GOOG SNPS . Date . 2012-05-22 -0.004070 | 0.003599 | -0.004589 | -0.007679 | 0.000336 | -0.012206 | 0.013178 | 0.013394 | -0.001701 | -0.001952 | 0.012295 | 0.002684 | 0.000000 | -0.089039 | -0.022222 | -0.012746 | 0.006966 | -0.004735 | -0.021674 | 0.000000 | . 2012-05-23 -0.007493 | -0.002391 | -0.022666 | 0.024400 | -0.021842 | 0.024712 | -0.011189 | 0.008573 | 0.001874 | -0.003352 | 0.005992 | 0.007762 | 0.005311 | 0.032258 | -0.012987 | 0.009056 | 0.008828 | 0.006244 | 0.014414 | 0.005706 | . 2012-05-24 -0.008579 | -0.017975 | 0.008255 | -0.009184 | -0.001374 | -0.026527 | 0.017916 | -0.001417 | -0.018707 | 0.008969 | -0.006922 | -0.029214 | -0.019888 | 0.032187 | -0.009868 | -0.009389 | 0.006569 | -0.059988 | -0.009517 | 0.058511 | . 2012-05-25 0.001731 | -0.003661 | 0.003509 | -0.005360 | -0.000344 | 0.023947 | -0.016674 | 0.001064 | -0.010919 | 0.005278 | 0.006484 | 0.005403 | 0.001902 | -0.033909 | 0.033223 | -0.010918 | -0.013981 | -0.014775 | -0.020094 | 0.001340 | . 2012-05-29 0.014513 | 0.015922 | 0.013598 | 0.017749 | 0.017206 | 0.025806 | 0.023316 | 0.003189 | 0.020151 | 0.019619 | 0.015623 | -0.008912 | 0.014240 | -0.096208 | 0.038585 | 0.008737 | 0.005048 | 0.009253 | 0.004750 | 0.009368 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-12-23 0.002514 | 0.012189 | 0.006671 | 0.003644 | 0.004472 | 0.008163 | 0.008153 | 0.006412 | 0.004878 | 0.002148 | 0.007363 | 0.001345 | 0.010000 | 0.014495 | 0.015707 | 0.000184 | 0.008672 | 0.012912 | 0.001317 | 0.003618 | . 2021-12-27 0.023693 | 0.018304 | 0.012278 | 0.022975 | 0.023186 | 0.044028 | 0.024262 | 0.026836 | 0.008638 | 0.016275 | 0.018905 | 0.020384 | 0.014150 | 0.032633 | 0.056247 | -0.008178 | -0.000749 | -0.005219 | 0.006263 | 0.025782 | . 2021-12-28 -0.003857 | 0.001734 | -0.003466 | -0.005767 | -0.003504 | -0.020133 | -0.001184 | -0.004580 | 0.001557 | -0.006212 | -0.003161 | -0.011034 | -0.014402 | 0.000116 | -0.007839 | 0.005844 | 0.001304 | 0.010320 | -0.010914 | -0.009159 | . 2021-12-29 -0.001518 | 0.006768 | 0.001353 | 0.000502 | 0.002051 | -0.010586 | 0.003162 | -0.002693 | -0.010388 | 0.006537 | -0.006977 | -0.003562 | -0.000123 | -0.009474 | -0.031929 | -0.008555 | 0.001414 | 0.003405 | 0.000386 | 0.003466 | . 2021-12-30 -0.007337 | -0.005316 | -0.001736 | -0.006578 | -0.007691 | -0.013833 | 0.001182 | -0.007206 | 0.002571 | -0.004216 | -0.003390 | 0.003104 | 0.002178 | 0.004141 | -0.020977 | -0.003289 | -0.000830 | -0.005260 | -0.003427 | -0.007043 | . 2419 rows × 20 columns . I expected the stocks in this portfolio to have a large number of positive correlations considering they are all part of the same sector in fact no pair is negatively correlated . The top 3 most correlated stocks are: Analog devices and Texas Instruments which have a strong positive correlation with 0.80 both companies are in the business of designing and fabrication of semiconductors and semiconductor devices which is a sub-industry of the overall technology sector, . The other two sets of stocks have a moderate positive correlation which is Ansys Inc, and Synopsys inc with 0.69 and Synopsys inc again but this time with financial software company Intuit inc with 0.66 . corr = returns.corr() fig = px.imshow(corr) fig.update_layout(width=1000, height=800) fig.update_layout(template = &quot;plotly_dark&quot;, title = &#39;The Correlation coefficient of the Assets in the Portfolio&#39;) fig.show() . corr.unstack().sort_values().drop_duplicates() . FB AMD 0.234715 AMD INFY 0.235012 FB INFY 0.236786 INFY AMZN 0.248299 VMW AMD 0.264527 ... MSFT ADBE 0.662720 SNPS INTU 0.672901 ANSS SNPS 0.698040 ADI TXN 0.803991 TXN TXN 1.000000 Length: 191, dtype: float64 . Creating an equal weight (EW) portfolio: . Equal weight is a type of proportional measuring method that gives the same importance to each stock in a portfolio, index, or index fund. So stocks of the smallest companies are given equal statistical significance, or weight, to the largest companies when it comes to evaluating the overall group&#39;s performance. . N = len(returns.columns) equal_weights = N * [1/N] # Shows 1/20, 20 times. Its not multiplication, but repetition! 20*[&quot;A&quot;] equal_weights . [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05] . portfolio_return = returns.dot(equal_weights) portfolio_return . Date 2012-05-22 -0.006508 2012-05-23 0.003661 2012-05-24 -0.004807 2012-05-25 -0.002338 2012-05-29 0.008578 ... 2021-12-23 0.006743 2021-12-27 0.019035 2021-12-28 -0.004217 2021-12-29 -0.002838 2021-12-30 -0.004248 Length: 2419, dtype: float64 . pf_data.index dates = pf_data.index.to_frame().reset_index(drop=True) . The returns were noticeably volatile in 2018 November as that year a lot was happening like the federal reserve interest hike but the most notable event was a lot of the big tech were under scrutiny at the time and considering this is a tech portfolio the volatility shouldn’t be surprising . Another noticeable moment here is the pandemic in 2020, volatility was extremely high, in fact, On March 16, 2020, the VIX closed at a record high of 82.69 The markets were tumbling and a lot of trades were being made, some were covering short positions while others buying “the dip” and last but not least you have countless algorithims and retail traders day trading and taking advantage of the high volatility . fig = go.Figure() fig.add_trace(go.Scatter(x=dates[&#39;Date&#39;], y=portfolio_return, mode=&#39;lines&#39;, line=dict(color=&#39;firebrick&#39;,width=2), name=&#39;lines&#39;)) fig.update_layout(template = &quot;plotly_dark&quot;) display(HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;))); . cum_equal_returns = (1 + portfolio_return).cumprod() - 1 cum_equal_returns_perc = pd.Series(100 * cum_equal_returns) . The EW has done pretty well returning more than 1000%! . fig = go.Figure([go.Scatter(x=dates[&#39;Date&#39;], y=cum_equal_returns_perc)]) fig.update_layout(template = &quot;plotly_dark&quot;, title = &#39;Cummulative % Return&#39;) fig.show() display(HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;))); . Sharpe Ratio . In the next step i am going to calcluate sharpe ratio, but first we need the annual volatility, annual return and risk free rate . ER = portfolio_return.mean() STD = portfolio_return.std() . ASTD = STD * 252 ** 0.5 ASTD . 0.21152541130551866 . AER = ER * 252 AER . 0.2784416783450839 . rf = 0.03 #risk free rate is the 10 year trasury bond as of april 2022 excess_return = AER - rf SR = excess_return/ASTD SR . 1.1745240291070507 . A Sharpe ratio of 1.17 is not the best but also considering that tech stocks are very volatile maybe there is a weight combination that would have a higher Sharpe ratio and/or lower volatility or even a higher expected return and that is what I’ll try to uncover in the next section . Modern Portfolio Theory &amp; Monte Carlo simulation: . Monte Carlo simulations are used to model the probability of different outcomes in a process that cannot easily be predicted due to the intervention of random variables. It is a technique used to understand the impact of risk and uncertainty in prediction and forecasting models. . Modern portfolio theory refers to the quantitative practice of asset allocation that maximizes projected (ex ante) return for a portfolio while holding constant its overall exposure to risk. Or, inversely, minimizing overall risk for a given target portfolio return. The theory considers the covariance of constituent assets or asset classes within a portfolio, and the impact of an asset allocation change on the overall expected risk/return profile of the portfolio. The theory was originally proposed by nobel-winning economist Harry Markowitz in the 1952 Journal of Finance, and is now a cornerstone of portfolio management practice. Modern portfolio theory generally supports a practice of diversifying toward a mix of assets and asset classes with a low degree of mutual correlation. . Hence, I’m going to find the optimal portfolio using Monte Carlo simulations by building thousands of portfolios, using randomly assigned weights, and visualizing the results. . num_assets = len(pf_data.columns) num_assets . 20 . log_returns = np.log(pf_data/pf_data.shift(1)) log_returns.head() . TXN CSCO INTC AAPL MSFT NVDA INFY INTU SAP ADI ANSS CRM ADBE FB AMD AMZN MA VMW GOOG SNPS . Date . 2012-05-21 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2012-05-22 -0.004079 | 0.003593 | -0.004599 | -0.007709 | 0.000336 | -0.012280 | 0.013093 | 0.013305 | -0.001702 | -0.001953 | 0.012220 | 0.002680 | 0.000000 | -0.093255 | -0.022473 | -0.012828 | 0.006942 | -0.004746 | -0.021912 | 0.000000 | . 2012-05-23 -0.007521 | -0.002394 | -0.022927 | 0.024107 | -0.022083 | 0.024411 | -0.011252 | 0.008536 | 0.001873 | -0.003358 | 0.005974 | 0.007732 | 0.005297 | 0.031749 | -0.013072 | 0.009015 | 0.008790 | 0.006225 | 0.014311 | 0.005690 | . 2012-05-24 -0.008616 | -0.018139 | 0.008221 | -0.009227 | -0.001375 | -0.026885 | 0.017758 | -0.001418 | -0.018885 | 0.008929 | -0.006946 | -0.029650 | -0.020089 | 0.031680 | -0.009917 | -0.009433 | 0.006548 | -0.061863 | -0.009562 | 0.056863 | . 2012-05-25 0.001729 | -0.003668 | 0.003502 | -0.005374 | -0.000344 | 0.023665 | -0.016815 | 0.001063 | -0.010979 | 0.005264 | 0.006463 | 0.005389 | 0.001901 | -0.034497 | 0.032683 | -0.010978 | -0.014080 | -0.014886 | -0.020299 | 0.001339 | . cov = log_returns.cov() cov . TXN CSCO INTC AAPL MSFT NVDA INFY INTU SAP ADI ANSS CRM ADBE FB AMD AMZN MA VMW GOOG SNPS . TXN 0.000283 | 0.000149 | 0.000206 | 0.000162 | 0.000165 | 0.000267 | 0.000120 | 0.000163 | 0.000135 | 0.000244 | 0.000171 | 0.000170 | 0.000175 | 0.000147 | 0.000268 | 0.000141 | 0.000157 | 0.000150 | 0.000140 | 0.000161 | . CSCO 0.000149 | 0.000248 | 0.000156 | 0.000132 | 0.000144 | 0.000181 | 0.000104 | 0.000137 | 0.000117 | 0.000147 | 0.000131 | 0.000150 | 0.000143 | 0.000112 | 0.000170 | 0.000115 | 0.000141 | 0.000148 | 0.000117 | 0.000125 | . INTC 0.000206 | 0.000156 | 0.000349 | 0.000157 | 0.000176 | 0.000243 | 0.000121 | 0.000161 | 0.000134 | 0.000202 | 0.000161 | 0.000155 | 0.000166 | 0.000145 | 0.000229 | 0.000131 | 0.000152 | 0.000150 | 0.000138 | 0.000152 | . AAPL 0.000162 | 0.000132 | 0.000157 | 0.000317 | 0.000167 | 0.000223 | 0.000107 | 0.000154 | 0.000123 | 0.000165 | 0.000157 | 0.000161 | 0.000167 | 0.000169 | 0.000218 | 0.000154 | 0.000150 | 0.000128 | 0.000148 | 0.000145 | . MSFT 0.000165 | 0.000144 | 0.000176 | 0.000167 | 0.000262 | 0.000233 | 0.000109 | 0.000178 | 0.000135 | 0.000164 | 0.000170 | 0.000189 | 0.000199 | 0.000160 | 0.000212 | 0.000168 | 0.000162 | 0.000147 | 0.000168 | 0.000161 | . NVDA 0.000267 | 0.000181 | 0.000243 | 0.000223 | 0.000233 | 0.000664 | 0.000142 | 0.000232 | 0.000170 | 0.000276 | 0.000236 | 0.000260 | 0.000262 | 0.000227 | 0.000473 | 0.000216 | 0.000208 | 0.000203 | 0.000201 | 0.000231 | . INFY 0.000120 | 0.000104 | 0.000121 | 0.000107 | 0.000109 | 0.000142 | 0.000334 | 0.000124 | 0.000107 | 0.000121 | 0.000116 | 0.000124 | 0.000119 | 0.000100 | 0.000159 | 0.000085 | 0.000124 | 0.000108 | 0.000096 | 0.000107 | . INTU 0.000163 | 0.000137 | 0.000161 | 0.000154 | 0.000178 | 0.000232 | 0.000124 | 0.000282 | 0.000132 | 0.000166 | 0.000182 | 0.000196 | 0.000202 | 0.000159 | 0.000213 | 0.000146 | 0.000178 | 0.000143 | 0.000149 | 0.000176 | . SAP 0.000135 | 0.000117 | 0.000134 | 0.000123 | 0.000135 | 0.000170 | 0.000107 | 0.000132 | 0.000257 | 0.000138 | 0.000136 | 0.000152 | 0.000150 | 0.000122 | 0.000181 | 0.000119 | 0.000139 | 0.000134 | 0.000118 | 0.000124 | . ADI 0.000244 | 0.000147 | 0.000202 | 0.000165 | 0.000164 | 0.000276 | 0.000121 | 0.000166 | 0.000138 | 0.000325 | 0.000178 | 0.000179 | 0.000183 | 0.000151 | 0.000277 | 0.000135 | 0.000167 | 0.000151 | 0.000142 | 0.000168 | . ANSS 0.000171 | 0.000131 | 0.000161 | 0.000157 | 0.000170 | 0.000236 | 0.000116 | 0.000182 | 0.000136 | 0.000178 | 0.000286 | 0.000198 | 0.000202 | 0.000150 | 0.000226 | 0.000154 | 0.000166 | 0.000150 | 0.000147 | 0.000184 | . CRM 0.000170 | 0.000150 | 0.000155 | 0.000161 | 0.000189 | 0.000260 | 0.000124 | 0.000196 | 0.000152 | 0.000179 | 0.000198 | 0.000434 | 0.000248 | 0.000214 | 0.000246 | 0.000192 | 0.000187 | 0.000192 | 0.000169 | 0.000179 | . ADBE 0.000175 | 0.000143 | 0.000166 | 0.000167 | 0.000199 | 0.000262 | 0.000119 | 0.000202 | 0.000150 | 0.000183 | 0.000202 | 0.000248 | 0.000340 | 0.000192 | 0.000249 | 0.000190 | 0.000177 | 0.000170 | 0.000171 | 0.000189 | . FB 0.000147 | 0.000112 | 0.000145 | 0.000169 | 0.000160 | 0.000227 | 0.000100 | 0.000159 | 0.000122 | 0.000151 | 0.000150 | 0.000214 | 0.000192 | 0.000521 | 0.000201 | 0.000199 | 0.000161 | 0.000141 | 0.000186 | 0.000149 | . AMD 0.000268 | 0.000170 | 0.000229 | 0.000218 | 0.000212 | 0.000473 | 0.000159 | 0.000213 | 0.000181 | 0.000277 | 0.000226 | 0.000246 | 0.000249 | 0.000201 | 0.001315 | 0.000217 | 0.000204 | 0.000211 | 0.000167 | 0.000230 | . AMZN 0.000141 | 0.000115 | 0.000131 | 0.000154 | 0.000168 | 0.000216 | 0.000085 | 0.000146 | 0.000119 | 0.000135 | 0.000154 | 0.000192 | 0.000190 | 0.000199 | 0.000217 | 0.000350 | 0.000152 | 0.000132 | 0.000178 | 0.000141 | . MA 0.000157 | 0.000141 | 0.000152 | 0.000150 | 0.000162 | 0.000208 | 0.000124 | 0.000178 | 0.000139 | 0.000167 | 0.000166 | 0.000187 | 0.000177 | 0.000161 | 0.000204 | 0.000152 | 0.000276 | 0.000147 | 0.000154 | 0.000154 | . VMW 0.000150 | 0.000148 | 0.000150 | 0.000128 | 0.000147 | 0.000203 | 0.000108 | 0.000143 | 0.000134 | 0.000151 | 0.000150 | 0.000192 | 0.000170 | 0.000141 | 0.000211 | 0.000132 | 0.000147 | 0.000471 | 0.000127 | 0.000134 | . GOOG 0.000140 | 0.000117 | 0.000138 | 0.000148 | 0.000168 | 0.000201 | 0.000096 | 0.000149 | 0.000118 | 0.000142 | 0.000147 | 0.000169 | 0.000171 | 0.000186 | 0.000167 | 0.000178 | 0.000154 | 0.000127 | 0.000250 | 0.000135 | . SNPS 0.000161 | 0.000125 | 0.000152 | 0.000145 | 0.000161 | 0.000231 | 0.000107 | 0.000176 | 0.000124 | 0.000168 | 0.000184 | 0.000179 | 0.000189 | 0.000149 | 0.000230 | 0.000141 | 0.000154 | 0.000134 | 0.000135 | 0.000241 | . num_ports = 20000 #the number of trials i will run all_weights = np.zeros((num_ports,num_assets)) ret_arr = np.zeros(num_ports) vol_arr = np.zeros(num_ports) sharpe_arr = np.zeros(num_ports) for ind in range(num_ports): #weigths weights = np.array(np.random.random(num_assets)) weights = weights/np.sum(weights) #save weigths all_weights[ind,:] = weights #expected return ret_arr[ind] = np.sum((log_returns.mean() * weights) * 250) #expected volatility vol_arr[ind] = np.sqrt(np.dot(weights.T,np.dot(log_returns.cov()*250,weights))) #sharpe ratio sharpe_arr[ind] = (ret_arr[ind] - rf)/vol_arr[ind] . After the monte carlo is done it&#39;s time to inspect and locate the results and look at the weightings of the portfolios we need a portfolio that might be better than my initial equal weighted portfolio, rememebr returns alone are not the objective but also volatility, we want the highest return for the lowest volatility possible hence the highest sharpe ratio . In the next step i will be creating a data frame that will contain not just the weigthings but also the expected return, volatility and even the sharpe ratio of all the portfolios generated which will help me locate where the optimal or tangency portfolio, the portfolio with minimum volatility and the portfolio with maximum expected return are at in the weightings that were generated . data = pd.DataFrame({&#39;Return&#39;: ret_arr, &#39;Volatility&#39;: vol_arr, &#39;Sharpe Ratio&#39;: sharpe_arr})#(ret_arr - rf) /vol_arr}) for counter, symbol in enumerate(pf_data.columns.tolist()): data[symbol + &#39;weight&#39;] = [w[counter]for w in all_weights] portfolios = pd.DataFrame(data) portfolios . Return Volatility Sharpe Ratio TXNweight CSCOweight INTCweight AAPLweight MSFTweight NVDAweight INFYweight ... ANSSweight CRMweight ADBEweight FBweight AMDweight AMZNweight MAweight VMWweight GOOGweight SNPSweight . 0 0.235060 | 0.213741 | 0.959388 | 0.061536 | 0.091934 | 0.040965 | 0.067888 | 0.076020 | 0.054105 | 0.049063 | ... | 0.003004 | 0.092469 | 0.040359 | 0.081557 | 0.032928 | 0.003992 | 0.066299 | 0.014090 | 0.053930 | 0.019123 | . 1 0.229757 | 0.219309 | 0.910850 | 0.061311 | 0.031462 | 0.050643 | 0.049502 | 0.001270 | 0.077914 | 0.029312 | ... | 0.033376 | 0.040043 | 0.078451 | 0.053226 | 0.073149 | 0.071961 | 0.062006 | 0.077717 | 0.030547 | 0.007948 | . 2 0.239854 | 0.218571 | 0.960119 | 0.022931 | 0.034186 | 0.000097 | 0.095811 | 0.055255 | 0.098486 | 0.032107 | ... | 0.039784 | 0.038417 | 0.094715 | 0.038167 | 0.052099 | 0.029348 | 0.002343 | 0.069246 | 0.053960 | 0.017094 | . 3 0.223821 | 0.206283 | 0.939588 | 0.021978 | 0.088269 | 0.028764 | 0.031951 | 0.023287 | 0.044314 | 0.061802 | ... | 0.084773 | 0.025944 | 0.023086 | 0.070774 | 0.019001 | 0.004489 | 0.067659 | 0.011433 | 0.067834 | 0.080970 | . 4 0.252654 | 0.220076 | 1.011718 | 0.032829 | 0.038141 | 0.031621 | 0.038156 | 0.032453 | 0.071135 | 0.023667 | ... | 0.049051 | 0.001107 | 0.104740 | 0.069692 | 0.071773 | 0.051470 | 0.017169 | 0.034197 | 0.086193 | 0.106864 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 19995 0.236976 | 0.217382 | 0.952134 | 0.065015 | 0.062946 | 0.031630 | 0.024278 | 0.054483 | 0.060273 | 0.038499 | ... | 0.071690 | 0.029728 | 0.083362 | 0.078441 | 0.073044 | 0.005775 | 0.036472 | 0.059114 | 0.075507 | 0.040374 | . 19996 0.211920 | 0.207550 | 0.876511 | 0.059174 | 0.066972 | 0.080858 | 0.028952 | 0.050625 | 0.050136 | 0.074528 | ... | 0.066560 | 0.097057 | 0.004428 | 0.037434 | 0.029613 | 0.009752 | 0.050356 | 0.032781 | 0.063095 | 0.061517 | . 19997 0.218922 | 0.215106 | 0.878273 | 0.020343 | 0.000088 | 0.041725 | 0.001719 | 0.004495 | 0.014127 | 0.094459 | ... | 0.033807 | 0.010043 | 0.056615 | 0.023852 | 0.106904 | 0.110114 | 0.094245 | 0.093785 | 0.106817 | 0.022204 | . 19998 0.231003 | 0.213849 | 0.939929 | 0.070349 | 0.038615 | 0.025927 | 0.055969 | 0.053822 | 0.084375 | 0.079402 | ... | 0.067857 | 0.047777 | 0.030630 | 0.029430 | 0.042781 | 0.011784 | 0.023835 | 0.074925 | 0.056096 | 0.032982 | . 19999 0.214351 | 0.207301 | 0.889289 | 0.038387 | 0.083181 | 0.062824 | 0.052065 | 0.058076 | 0.032348 | 0.044598 | ... | 0.071942 | 0.011572 | 0.003619 | 0.072106 | 0.048186 | 0.023791 | 0.094538 | 0.051598 | 0.099134 | 0.007197 | . 20000 rows × 23 columns . The tangency portfolio: . The tangency or maximum Sharpe ratio portfolio in the Markowitz procedure possesses the highest potential return-for-risk tradeoff. . optimal_risky_portfolio = portfolios.iloc[portfolios[&#39;Sharpe Ratio&#39;].idxmax()] optimal_risky_portfolio . Return 0.263745 Volatility 0.215477 Sharpe Ratio 1.084784 TXNweight 0.041522 CSCOweight 0.060248 INTCweight 0.001923 AAPLweight 0.089765 MSFTweight 0.099979 NVDAweight 0.083038 INFYweight 0.066468 INTUweight 0.103850 SAPweight 0.026235 ADIweight 0.014330 ANSSweight 0.024099 CRMweight 0.008061 ADBEweight 0.076826 FBweight 0.000352 AMDweight 0.056392 AMZNweight 0.091473 MAweight 0.010688 VMWweight 0.009695 GOOGweight 0.061586 SNPSweight 0.073470 Name: 13834, dtype: float64 . Minim vol is also known as the minimum variance portfolio: . The minimum variance portfolio (mvp) is the portfolio that provides the lowest variance (standard deviation) among all possible portfolios of risky assets. . min_vol_port = portfolios.iloc[portfolios[&#39;Volatility&#39;].idxmin()] min_vol_port . Return 0.202356 Volatility 0.197514 Sharpe Ratio 0.872627 TXNweight 0.042340 CSCOweight 0.105379 INTCweight 0.077549 AAPLweight 0.026698 MSFTweight 0.002516 NVDAweight 0.014645 INFYweight 0.102012 INTUweight 0.034337 SAPweight 0.058483 ADIweight 0.076789 ANSSweight 0.010081 CRMweight 0.010085 ADBEweight 0.006261 FBweight 0.039513 AMDweight 0.017248 AMZNweight 0.033863 MAweight 0.028866 VMWweight 0.053479 GOOGweight 0.132173 SNPSweight 0.127684 Name: 5947, dtype: float64 . Max return portfolio: The portfolio with the highest return regardless of risk . max_er_port = portfolios.iloc[portfolios[&#39;Return&#39;].idxmax()] max_er_port . Return 0.267209 Volatility 0.228768 Sharpe Ratio 1.036900 TXNweight 0.086701 CSCOweight 0.024911 INTCweight 0.005748 AAPLweight 0.085139 MSFTweight 0.021823 NVDAweight 0.135536 INFYweight 0.039539 INTUweight 0.023250 SAPweight 0.006508 ADIweight 0.000708 ANSSweight 0.064148 CRMweight 0.091271 ADBEweight 0.014365 FBweight 0.006770 AMDweight 0.076369 AMZNweight 0.109782 MAweight 0.038671 VMWweight 0.031825 GOOGweight 0.061691 SNPSweight 0.075245 Name: 6471, dtype: float64 . With a scatter plot I’ll be able to visually see the portfolios and where they lay on the frontier, but remember the correlation of the stocks, there was virtually no negative correlation and modern portfolio theory is about diversifying with UNCORELATED assets, hence I do not expect the plot to form the usual bullet like shape, but i will still be able to see the optimal portfolios across the edges of the fronteir . plt.figure(figsize=(20,10)) plt.scatter(portfolios[&#39;Volatility&#39;],portfolios[&#39;Return&#39;],c=sharpe_arr,cmap=&#39;RdBu&#39;)#ret_arr,vol_arr plt.colorbar(label=&#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Risk (Volatility)&#39;) plt.ylabel(&#39;Expected Returns&#39;) plt.scatter(optimal_risky_portfolio[1],optimal_risky_portfolio[0], c=&#39;green&#39;, s=80) plt.scatter(min_vol_port[1],min_vol_port[0], c=&#39;purple&#39;, s=80)# plt.scatter(max_er_port[1],max_er_port[0], c=&#39;yellow&#39;, s=80) plt.style.use(&#39;dark_background&#39;) display(HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;))); . C: Users one AppData Roaming Python Python37 site-packages ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first. . Now that we know the weights of each portfolio, I&#39;ll Assign the weights to the stocks and check the cumulative returns of each of the portfolios . But, NOTE: You’ve might have noticed from the observations produced by the simulation, the tangency portfolio has a lower sharp than my initial equal weight portfolio which now means based on ALL the observations I have, the EW portfolio is my tangency/Optimal portfolio and i will treat it as my optimal portfolio going forward . min_vol_weights = all_weights[5947,:] min_vol_weights . array([0.04234009, 0.10537944, 0.07754858, 0.02669783, 0.00251591, 0.0146454 , 0.10201163, 0.03433662, 0.0584827 , 0.07678887, 0.0100807 , 0.0100849 , 0.00626119, 0.03951333, 0.01724793, 0.03386276, 0.02886554, 0.05347914, 0.13217338, 0.12768405]) . min_vol_port_return = returns.dot(min_vol_weights) cum_minvol_returns = (1 + min_vol_port_return).cumprod() - 1 cum_minvol_returns_perc = pd.Series(100 * cum_minvol_returns) #Plot fig = go.Figure([go.Scatter(x=dates[&#39;Date&#39;], y=cum_minvol_returns_perc)]) fig.update_layout(template = &quot;plotly_dark&quot;, title = &#39;Cummulative % Return of the minimum variance portfolio&#39;) fig.show() display(HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;))); . max_er_weights = all_weights[6471,:] max_er_weights . array([0.08670101, 0.02491136, 0.00574761, 0.08513945, 0.02182338, 0.13553581, 0.03953875, 0.02325034, 0.00650826, 0.00070845, 0.06414761, 0.09127111, 0.01436485, 0.00676964, 0.07636874, 0.1097819 , 0.03867063, 0.03182504, 0.06169064, 0.07524542]) . max_er_port_return = returns.dot(max_er_weights) cum_maxer_returns = (1 + max_er_port_return).cumprod() - 1 cum_maxer_returns_perc = pd.Series(100 * cum_maxer_returns) #Plot fig = go.Figure([go.Scatter(x=dates[&#39;Date&#39;], y=cum_maxer_returns_perc)]) fig.update_layout(template = &quot;plotly_dark&quot;, title = &#39;Cummulative % Return of the maximum expected return portfolio&#39;) fig.show() display(HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;))); . Portfolio vs benchmarks: . It’s time to compare the all the portfolios against certain benchmarks which are going to be the Invesco QQQ fund which is a technology ETF, I chose this particular tech ETF as a benchmark because it has the highest NAV of 135 billion as of April 2022 . I&#39;ll also include the NASDAQ composite as a benchmark as it is widely followed and considered as a benchmark by investors, the Nasdaq composite is even more relevant here because more than 50% of the stocks in the index are technology companies . QQQ = pd.DataFrame(yf.download(&#39;QQQ&#39;, start=&quot;2012-05-20&quot;, end=&quot;2021-12-31&quot;, index_col = &#39;Date&#39;, parse_dates=True)[&#39;Adj Close&#39;]) NASDAQ = pd.DataFrame(yf.download(&#39;^IXIC&#39;, start=&quot;2012-05-20&quot;, end=&quot;2021-12-31&quot;, index_col = &#39;Date&#39;, parse_dates=True)[&#39;Adj Close&#39;]) . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . for funds in (QQQ,NASDAQ): funds[&#39;Daily Return&#39;] = funds.pct_change(1).dropna() funds[&#39;Cumulative Return&#39;] = (1 + funds[&#39;Daily Return&#39;]).cumprod() - 1 funds[&#39;Cumulative % Return&#39;] = funds[&#39;Cumulative Return&#39;] * 100 #creating a data frame that has all the portfolios and benchmarks cummulative % return data = {&#39;QQQ&#39;:QQQ[&#39;Cumulative % Return&#39;], &#39;NASDAQ&#39;:NASDAQ[&#39;Cumulative % Return&#39;], &#39;Optimal Port&#39;:cum_equal_returns_perc, &#39;Min Variance&#39;:cum_minvol_returns_perc, &#39;Max ER Port&#39;:cum_maxer_returns_perc} funds_cumm = pd.DataFrame(data) funds_cumm.reset_index(drop=True, inplace=True) funds_cumm.insert(loc=0, column=&quot;Dates&quot;, value=dates) funds_cumm.tail() . Dates QQQ NASDAQ Optimal Port Min Variance Max ER Port . 2415 2021-12-23 | 598.161713 | 449.779269 | 1057.179336 | 765.628694 | 1652.657742 | . 2416 2021-12-27 | 609.700431 | 457.432012 | 1079.205886 | 780.031449 | 1690.668661 | . 2417 2021-12-28 | 606.411117 | 454.287177 | 1074.233314 | 776.898982 | 1679.724874 | . 2418 2021-12-29 | 606.305615 | 453.742785 | 1070.900768 | 777.018843 | 1670.698787 | . 2419 2021-12-30 | 604.194880 | 452.876668 | 1065.927132 | 773.809387 | 1659.601642 | . Both benchmarks performed poorly compared to the portfolios but this is largely due to concentration, as the portfolios have only 20 stocks while the benchmarks usually have hundreds of stocks in them, but the portfolio&#39;s strongest point is also its weakest as a concentrated portfolio will probably have a much larger drawdown even during corrections let alone recessions . The max expected return portfolio outperformed all the other portfolios and the benchmarks as expected . While my initial EW portfolio now turned into my tangency optimal portfolio faired well by beating both the benchmarks by a mile! . fig = px.line(funds_cumm, x=&quot;Dates&quot;, y=funds_cumm.columns, hover_data={&quot;Dates&quot;: &quot;|%B %d, %Y&quot;}, title=&#39;Commulative % Return&#39;) fig.update_xaxes( rangeslider_visible=True, rangeselector=dict( buttons=list([ dict(count=1, label=&quot;1m&quot;, step=&quot;month&quot;, stepmode=&quot;backward&quot;), dict(count=6, label=&quot;6m&quot;, step=&quot;month&quot;, stepmode=&quot;backward&quot;), dict(count=1, label=&quot;YTD&quot;, step=&quot;year&quot;, stepmode=&quot;todate&quot;), dict(count=1, label=&quot;1y&quot;, step=&quot;year&quot;, stepmode=&quot;backward&quot;), dict(step=&quot;all&quot;) ]) ) ) fig.update_layout(template = &quot;plotly_dark&quot;, title = &#39;10 years Cummulative % Return of all tech portfolios and benchmarks&#39;) fig.show() display(HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;))); . Conclusion: . Assuming I started with a $5,000 in 2012 and invested in the ideal two best portfolios Max return portfolio and Optimal portfolio, by comparison, how much would my investment be by the end of 2021? (without rebalancing) . The max expected return portfolio seems to have a better outcome and seems very attractive especially considering the Sharpe ratio difference isn&#39;t that big but this portfolio was picked because of its high expected return unfortunately, it&#39;s not practical due to the presence of large estimation errors in those expected return estimate. As I have estimated them using historical data and have assumed past performance will be the same in the future which is unlikely as businesses change ESPECIALLY in the ever-changing technology industry . Hence the more reliable portfolio based on this research would either be the min vol or EW/Tangency portfolio . Initial_Investment = 5000 #Minimum Variance Min_ASRr = str(round(portfolios[&#39;Sharpe Ratio&#39;][5947],2)) Min_AERr = str(round(portfolios[&#39;Return&#39;][5947]* 100,2)) + &#39;%&#39; Min_ASTDr = str(round(portfolios[&#39;Volatility&#39;].min()*100,2)) + &#39;%&#39; cumm = str(round(cum_minvol_returns_perc[2418],2)) + &#39;%&#39; EW_Value = Initial_Investment * (cum_minvol_returns_perc[2418]/100) Absolute_Value = &#39;$&#39; + str(round(EW_Value,2)) print(&#39;THE MINIMUM VARIANCE PORTFOLIO:&#39;) print(f&#39;The annual sharpe ratio of the minimum variance portfolio is {Min_ASRr}&#39;) print(f&#39;The annual Volatility of the minimum variance portfolio is {Min_ASTDr}&#39;) print(f&#39;The annual Expected Return of the minimum variance portfolio is {Min_AERr}&#39;) print(f&#39;The 10 yr cummulative return of the minimum variance portfolio is {cumm}&#39;) print(f&#39;A ${Initial_Investment} investment in minimum variance portfolio in 2012 would be worth {Absolute_Value} by the end of 2021&#39;) #Tangency/Optimal Portfolio ASRr = str(round(SR,2)) AERr = str(round(AER* 100,2)) + &#39;%&#39; ASTDr = str(round(ASTD*100,2)) + &#39;%&#39; cumm3 = str(round(cum_equal_returns_perc[2418],2)) + &#39;%&#39; EW_Value3 = Initial_Investment * (cum_equal_returns_perc[2418]/100) Absolute_Value3 = &#39;$&#39; + str(round(EW_Value3,2)) print(&#39; nTHE OPTIMAL PORTFOLIO:&#39;) print(f&#39;The annual sharpe ratio of the optimal portfolio is {ASRr}&#39;) print(f&#39;The annual Volatility of the optimal portfolio is {ASTDr}&#39;) print(f&#39;The annual Expected Return of the optimal portfolio is {AERr}&#39;) print(f&#39;The 10 yr cummulative return of the optimal portfolio is {cumm3}&#39;) print(f&#39;A ${Initial_Investment} investment in the optimal portfolio in 2012 would be worth {Absolute_Value3} by the end of 2021&#39;) . THE MINIMUM VARIANCE PORTFOLIO: The annual sharpe ratio of the minimum variance portfolio is 0.87 The annual Volatility of the minimum variance portfolio is 19.75% The annual Expected Return of the minimum variance portfolio is 20.24% The 10 yr cummulative return of the minimum variance portfolio is 773.81% A $5000 investment in minimum variance portfolio in 2012 would be worth $38690.47 by the end of 2021 THE OPTIMAL PORTFOLIO: The annual sharpe ratio of the optimal portfolio is 1.17 The annual Volatility of the optimal portfolio is 21.15% The annual Expected Return of the optimal portfolio is 27.84% The 10 yr cummulative return of the optimal portfolio is 1065.93% A $5000 investment in the optimal portfolio in 2012 would be worth $53296.36 by the end of 2021 .",
            "url": "https://mjabubakar22.github.io/Monewmetrics/quantitative%20research/2022/05/07/Tech-Stocks-Portfolio.html",
            "relUrl": "/quantitative%20research/2022/05/07/Tech-Stocks-Portfolio.html",
            "date": " • May 7, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Footlocker Inc Stock Analysis and Valuation",
            "content": "Summary . The Stock Is Already Up 6% This Year. | The Sneaker Culture Is Not An Ephemeral Trend. | A Low P/E and High Shareholder Yield Makes It An Attractive Investment. | . Read the full analysis on seeking alpha . Download the full excel model . .",
            "url": "https://mjabubakar22.github.io/Monewmetrics/financial%20analysis/2021/01/21/Footlocker-Inc.html",
            "relUrl": "/financial%20analysis/2021/01/21/Footlocker-Inc.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Microsoft Corporation Stock Analysis and Valuation",
            "content": "Summary . AZURE growing faster than AWS and GCP. | Saas is at the center of the digital economy. | The Stock Price could potential grow by more than 40%. | . Read the full analysis on seeking alpha . Download the full excel model . .",
            "url": "https://mjabubakar22.github.io/Monewmetrics/financial%20analysis/2020/12/25/Microsoft-Corp.html",
            "relUrl": "/financial%20analysis/2020/12/25/Microsoft-Corp.html",
            "date": " • Dec 25, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Amdocs Limited Stock Analysis and Valuation",
            "content": "Summary . Amdocs role as a vendor is critical for CSP’s to exploit new 5G and IoT opportunities. | B2C and B2B markets are the key areas for 5G IoT connectivity growth. | COVID-19 makes IoT a necessity to the most impacted industries. | Amdocs top line to reach $8 Billion by 2030 with rich margins to create a huge influx of cash flow. | . Read the full analysis on seeking alpha . Download the full excel model . .",
            "url": "https://mjabubakar22.github.io/Monewmetrics/financial%20analysis/2020/12/21/Amdocs-LTD.html",
            "relUrl": "/financial%20analysis/2020/12/21/Amdocs-LTD.html",
            "date": " • Dec 21, 2020"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "About",
          "content": "As a Data Science and Finance enthusiast I enjoy working with data from importing, wrangling and analysis of data that produce actionable insights which leads to model development, evaluation, and deployment. . I come from an accounting and finance background, I don’t have a background in computer science or statistics nor do I have any professional experience as one. I ‘ve learned the basics through different books and courses but I’ve created and used Monewmetrics as a way to not only put to practice what I’ve learnt (and still learning) but also build up on that foundation, hence most of what you’ll encounter throughout the website will be me researching and learning different ideas, models and concepts for the first time . . Monewmetrics focuses on 3 areas: . Quantitative research: where I analyze (mostly financial) data and use advanced statistical models like the linear regression model or time series models like the autoregressive model to form predictions. Python is my preferred language since it has great libraries for data analysis, machine learning, and statistics. . Statistics &amp; probability: where I walk through different concepts from the basics like measures of location and dispersion of data to more advanced concepts like regression assumptions . Financial analysis: (through seeking alpha) where I do in-depth financial analysis on companies quantitatively like 3 statement (financial statement) modeling, scenario, and sensitivity analysis, DCF, etc, but also qualitatively like understanding company’s business model and competitive advantage, perform industry analysis and product market fit, etc. . I’m always open to feedback, you can reach out and/or connect with me on Linkedin! . DISCLAIMER! . Before proceeding, please make sure that you note the following important information: . NOT FINANCIAL ADVICE! My content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice, None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument to make any investment or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur. . This website is powered by **fastpages. .",
          "url": "https://mjabubakar22.github.io/Monewmetrics/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Categories",
          "content": ". Financial Analysis . Footlocker Inc Stock Analysis and Valuation . Microsoft Corporation Stock Analysis and Valuation . Amdocs Limited Stock Analysis and Valuation . . Quantitative Research . Stock Price Forecast Using ARIMA Model . BTC/USD Price Prediction Using Linear Regression . Portfolio Analysis, Efficient Frontier &amp; Monte Carlo .",
          "url": "https://mjabubakar22.github.io/Monewmetrics/categories/",
          "relUrl": "/categories/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mjabubakar22.github.io/Monewmetrics/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}